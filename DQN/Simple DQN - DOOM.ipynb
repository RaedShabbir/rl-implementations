{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raed/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from vizdoom import *\n",
    "\n",
    "\n",
    "from skimage import transform \n",
    "from skimage.color import rgb2gray \n",
    "\n",
    "from collections import deque \n",
    "\n",
    "import time \n",
    "import random \n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_env():\n",
    "    game = DoomGame()\n",
    "    game.load_config(\"basic.cfg\")\n",
    "    game.set_doom_scenario_path(\"basic.wad\")\n",
    "    game.init()\n",
    "    \n",
    "    left = [1,0,0]\n",
    "    shoot = [0,0,1]\n",
    "    right = [0,1,0]\n",
    "    \n",
    "    possible_actions = [left, right, shoot]\n",
    "    \n",
    "    return game, possible_actions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_env(test_eps = 10):\n",
    "    \"\"\"\n",
    "    Perform random action and test env to ensure it works \n",
    "    \"\"\"\n",
    "    game = DoomGame()\n",
    "    game.load_config(\"basic.cfg\")\n",
    "    game.set_doom_scenario_path(\"basic.wad\")\n",
    "    game.init()\n",
    "    \n",
    "    left = [1,0,0]\n",
    "    shoot = [0,0,1]\n",
    "    right = [0,1,0]\n",
    "    \n",
    "    possible_actions = [left, right, shoot]\n",
    "    \n",
    "    for i in range(test_eps):\n",
    "        game.new_episode()\n",
    "        while not game.is_episode_finished():\n",
    "            state = game.get_state()\n",
    "            img = state.screen_buffer\n",
    "            misc = state.game_variables\n",
    "            action = random.choice(possible_actions)\n",
    "            print (action)\n",
    "            \n",
    "            reward = game.make_action(action)\n",
    "            print (\"\\treward:\", reward)\n",
    "            \n",
    "            time.sleep(0.02)\n",
    "        \n",
    "        #prints every episode \n",
    "        print (\"Result:\", game.get_total_reward())\n",
    "        time.sleep(2)\n",
    "    \n",
    "    game.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call init_env \n",
    "game, possible_actions = init_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    \"\"\"\n",
    "    Convert frames to grayscale, crop irrelevent parts, normalize pixels, \n",
    "    resize frame.\n",
    "    \n",
    "    Input is frame of size 210x160x3\n",
    "    Returns frame of size 84x84x1 \n",
    "    \"\"\"\n",
    "    #graying done by vizdoom\n",
    "    #grayed = rgb2gray(frame)\n",
    "    \n",
    "    cropped = frame[30:-10, 30:-30]\n",
    "    normed = cropped/255.0 \n",
    "    resized = transform.resize(normed, [84,84])\n",
    "    \n",
    "    return resized #returns preprocessed frame \n",
    "\n",
    "stack_size = 4\n",
    "def stack_frames(stacked_frames, frame, is_new_episode, stack_size=stack_size):\n",
    "    \"\"\"\n",
    "    takes a frame/state and preprocesses it, \n",
    "    if same episode:\n",
    "        adds to the stack state \n",
    "    else new episode: \n",
    "        creates stacked state \n",
    "    \n",
    "    where the stacked state is 4 stacked states(frames)\n",
    "    returns stacked state where axis=1 is for different frames \n",
    "    \"\"\"\n",
    "    frame = preprocess_frame(frame)\n",
    "    \n",
    "    if not is_new_episode:\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "    \n",
    "    else: \n",
    "        #init deque \n",
    "        stacked_frames = deque([np.zeros((84,84), dtype=np.int) \n",
    "                                for i in range(stack_size)], maxlen=4)\n",
    "        #new episode so same frame x4 \n",
    "        for i in range(4): stacked_frames.append(frame)\n",
    "        \n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "\n",
    "    return stacked_state, stacked_frames\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 84, 4) (4, 84, 84)\n"
     ]
    }
   ],
   "source": [
    "#init frame stack\n",
    "stacked_frames  =  deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "frame = np.zeros((84,84), dtype=np.int)\n",
    "\n",
    "#test helper functions \n",
    "sample_state, stacked_frames = stack_frames(stacked_frames, frame, True)\n",
    "print(np.shape(sample_state), np.shape(stacked_frames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training params  \n",
    "total_episodes = 500 #total num of episodes \n",
    "total_test_episodes = 10 #total num of episodes to test on \n",
    "max_steps = 100 #max num of steps per episode \n",
    "bs = 64\n",
    "\n",
    "#network params \n",
    "lr = 0.0002 #learning rate \n",
    "state_size = [84,84,4]  #4 84x84 frames \n",
    "action_size = game.get_available_buttons_size() # 3 actions\n",
    "\n",
    "#discount factor \n",
    "gamma = 0.95 \n",
    "\n",
    "#exploration params \n",
    "epsilon = 1.0 #starting value for eps greedy (explore)\n",
    "max_eps = 1.0  #max value for eps greey \n",
    "min_eps = 0.01 #min value for eps greedy \n",
    "decay_rate = 0.0001 #decay rate for eps \n",
    "\n",
    "#Recall Params\n",
    "memory_size = 1000000\n",
    "pretrained_len = bs #number of init memories \n",
    "\n",
    "#Training Mode \n",
    "training=True \n",
    "\n",
    "#Env should be rendered or not \n",
    "episode_render = True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN: Model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='DQN', training=training):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.lr = learning_rate\n",
    "        self.training = training\n",
    "        with tf.variable_scope(name):\n",
    "            #placeholders\n",
    "            self.inputs = tf.placeholder(tf.float32,[None, *self.state_size])\n",
    "            self.actions = tf.placeholder(tf.float32, [None, self.action_size])\n",
    "            self.target_Q = tf.placeholder(tf.float32)\n",
    "            \n",
    "            #block 1 \n",
    "            self.conv1 = tf.layers.conv2d(inputs=self.inputs, filters=32, \n",
    "                                          kernel_size=[8,8], \n",
    "                                          strides=(4,4), \n",
    "                                          padding='valid', \n",
    "                                          kernel_initializer= tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            self.bn1 = tf.layers.batch_normalization(self.conv1, epsilon=1e-5, training=self.training)\n",
    "            self.elu1 = tf.nn.elu(self.bn1)\n",
    "            \n",
    "            #block 2\n",
    "            self.conv2 = tf.layers.conv2d(inputs=self.elu1, filters=64, \n",
    "                                          kernel_size=[4,4], \n",
    "                                          strides=(2,2), \n",
    "                                          padding='valid', \n",
    "                                          kernel_initializer= tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            self.bn2 = tf.layers.batch_normalization(self.conv2, epsilon=1e-5, training=self.training)\n",
    "            self.elu2 = tf.nn.elu(self.bn2)\n",
    "            \n",
    "            #block 3\n",
    "            self.conv3 = tf.layers.conv2d(inputs=self.elu2, filters=128, \n",
    "                                          kernel_size=[4,4], \n",
    "                                          strides=(2,2), \n",
    "                                          padding='valid', \n",
    "                                          kernel_initializer= tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            self.bn3 = tf.layers.batch_normalization(self.conv3, epsilon=1e-5, training=self.training)\n",
    "            self.elu3 = tf.nn.elu(self.bn3)\n",
    "            \n",
    "            #FC Block \n",
    "            self.flat = tf.layers.flatten(self.elu3)\n",
    "            self.fc = tf.layers.dense(self.flat, units=512, activation=tf.nn.elu, \n",
    "                                       kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.out = tf.layers.dense(self.fc, units=self.action_size, activation=None, \n",
    "                                       kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            #Q-Value prediction \n",
    "            self.pred_Q = tf.reduce_sum(tf.multiply(self.out, self.actions), axis=1)\n",
    "            \n",
    "            #Loss Function \n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_Q-self.pred_Q))\n",
    "            \n",
    "            #Optimizer \n",
    "            self.optim = tf.train.RMSPropOptimizer(learning_rate=self.lr).minimize(self.loss)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#init\n",
    "num_actions = len(possible_actions)\n",
    "DQNetwork = DQN(state_size, num_actions, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#memory class creates and manages deque \n",
    "\n",
    "class Memory():\n",
    "    def __init__(self, mem_limit):\n",
    "        self.cache = deque(maxlen=mem_limit)\n",
    "    \n",
    "    def add(self, exp):\n",
    "        self.cache.append(exp)\n",
    "    \n",
    "    def sample(self, sample_size):\n",
    "        sampling_ind = np.random.choice(np.arange(len(self.cache)), size=sample_size, replace=False)\n",
    "        return [self.cache[i] for i in sampling_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#activate and populate memory \n",
    "\n",
    "memory = Memory(mem_limit=memory_size)\n",
    "\n",
    "import pdb \n",
    "\n",
    "game.new_episode()\n",
    "\n",
    "for i in range(pretrained_len):\n",
    "    #pdb.set_trace()\n",
    "    #initilize state for first step  \n",
    "    if i == 0: \n",
    "        state = game.get_state().screen_buffer #init state \n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "    #take a random action\n",
    "    action = random.choice(possible_actions)\n",
    "    \n",
    "    #reward from chosen action \n",
    "    reward = game.make_action(action)\n",
    "    \n",
    "    #check if done \n",
    "    done = game.is_episode_finished()\n",
    "    \n",
    "    if not done:\n",
    "        #get next state \n",
    "        next_state = game.get_state().screen_buffer\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "        #add memory \n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        #update state\n",
    "        state = next_state\n",
    "        \n",
    "    else: #were dead\n",
    "        #update next state \n",
    "        next_state = np.zeros((84,84), dtype=np.int)\n",
    "\n",
    "        #add exp \n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "        #new episode and restart \n",
    "        game.new_episode()\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to predict action based on epsilon greedy \n",
    "\n",
    "def explore_exploit(epsilon, min_eps, decay_rate, decay_step, state, num_actions):\n",
    "    #produce random number between 0 and 1 \n",
    "    check = np.random.rand()\n",
    "    explore_prob = min_eps + (epsilon-min_eps)*np.exp(-decay_rate*decay_step)\n",
    "    \n",
    "    #explore\n",
    "    if explore_prob>check:\n",
    "        #take random action \n",
    "        action = random.choice(possible_actions)  \n",
    "        \n",
    "    #exploit \n",
    "    else:\n",
    "        #q vals predicted by network \n",
    "        Qs = sess.run(DQNetwork.out, feed_dict = {DQNetwork.inputs : state.reshape((1,*state.shape))})\n",
    "        #choose action corresponding to best Q value \n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[int(choice)]\n",
    "    return action, explore_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for saving \n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************\n",
      "Episode:\t 0\n",
      "Reward:\t -125.0\n",
      "Loss:\t 0.93442965\n",
      "Was Terminal?:\t False\n",
      "Model Saved at episode:  0\n",
      "*********************\n",
      "Episode:\t 1\n",
      "Reward:\t -125.0\n",
      "Loss:\t 0.8770241\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 2\n",
      "Reward:\t 89.0\n",
      "Loss:\t 0.55901897\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 3\n",
      "Reward:\t 68.0\n",
      "Loss:\t 153.41006\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 4\n",
      "Reward:\t -130.0\n",
      "Loss:\t 23.615196\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 5\n",
      "Reward:\t 91.0\n",
      "Loss:\t 15.830358\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  5\n",
      "*********************\n",
      "Episode:\t 6\n",
      "Reward:\t -125.0\n",
      "Loss:\t 3.9352164\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 7\n",
      "Reward:\t -125.0\n",
      "Loss:\t 5.265579\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 8\n",
      "Reward:\t 87.0\n",
      "Loss:\t 54.230595\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 9\n",
      "Reward:\t -130.0\n",
      "Loss:\t 3.2960236\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 10\n",
      "Reward:\t -125.0\n",
      "Loss:\t 4.1839294\n",
      "Was Terminal?:\t False\n",
      "Model Saved at episode:  10\n",
      "*********************\n",
      "Episode:\t 11\n",
      "Reward:\t 95.0\n",
      "Loss:\t 3.6666164\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 12\n",
      "Reward:\t -130.0\n",
      "Loss:\t 7.0721803\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 13\n",
      "Reward:\t 93.0\n",
      "Loss:\t 4.224107\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 14\n",
      "Reward:\t -125.0\n",
      "Loss:\t 6.372915\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 15\n",
      "Reward:\t 90.0\n",
      "Loss:\t 7.0016904\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  15\n",
      "*********************\n",
      "Episode:\t 16\n",
      "Reward:\t -125.0\n",
      "Loss:\t 2.3275473\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 17\n",
      "Reward:\t -130.0\n",
      "Loss:\t 4.79692\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 18\n",
      "Reward:\t 89.0\n",
      "Loss:\t 2.9417653\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 19\n",
      "Reward:\t -125.0\n",
      "Loss:\t 2.0693266\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 20\n",
      "Reward:\t -130.0\n",
      "Loss:\t 2.3763099\n",
      "Was Terminal?:\t False\n",
      "Model Saved at episode:  20\n",
      "*********************\n",
      "Episode:\t 21\n",
      "Reward:\t 93.0\n",
      "Loss:\t 2.3399525\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 22\n",
      "Reward:\t -125.0\n",
      "Loss:\t 4.319088\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 23\n",
      "Reward:\t -125.0\n",
      "Loss:\t 1.7662443\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 24\n",
      "Reward:\t 93.0\n",
      "Loss:\t 1.3119049\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 25\n",
      "Reward:\t 69.0\n",
      "Loss:\t 6.4597774\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  25\n",
      "*********************\n",
      "Episode:\t 26\n",
      "Reward:\t -125.0\n",
      "Loss:\t 1.3842189\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 27\n",
      "Reward:\t 95.0\n",
      "Loss:\t 2.5144475\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 28\n",
      "Reward:\t -130.0\n",
      "Loss:\t 1.3176486\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 29\n",
      "Reward:\t 46.0\n",
      "Loss:\t 1.97556\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 30\n",
      "Reward:\t -130.0\n",
      "Loss:\t 2.7895374\n",
      "Was Terminal?:\t False\n",
      "Model Saved at episode:  30\n",
      "*********************\n",
      "Episode:\t 31\n",
      "Reward:\t -130.0\n",
      "Loss:\t 3.906343\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 32\n",
      "Reward:\t -125.0\n",
      "Loss:\t 4.1210437\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 33\n",
      "Reward:\t -9.0\n",
      "Loss:\t 5.0475283\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 34\n",
      "Reward:\t 68.0\n",
      "Loss:\t 8.981951\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 35\n",
      "Reward:\t -125.0\n",
      "Loss:\t 4.06867\n",
      "Was Terminal?:\t False\n",
      "Model Saved at episode:  35\n",
      "*********************\n",
      "Episode:\t 36\n",
      "Reward:\t -120.0\n",
      "Loss:\t 4.361436\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 37\n",
      "Reward:\t 68.0\n",
      "Loss:\t 4.8126316\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 38\n",
      "Reward:\t 25.0\n",
      "Loss:\t 4.5510483\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 39\n",
      "Reward:\t -130.0\n",
      "Loss:\t 3.2050896\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 40\n",
      "Reward:\t 95.0\n",
      "Loss:\t 5.424183\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  40\n",
      "*********************\n",
      "Episode:\t 41\n",
      "Reward:\t 50.0\n",
      "Loss:\t 6.2493525\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 42\n",
      "Reward:\t 95.0\n",
      "Loss:\t 4.450964\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 43\n",
      "Reward:\t -130.0\n",
      "Loss:\t 2.8340127\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 44\n",
      "Reward:\t 22.0\n",
      "Loss:\t 7.9041615\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 45\n",
      "Reward:\t -125.0\n",
      "Loss:\t 5.2961445\n",
      "Was Terminal?:\t False\n",
      "Model Saved at episode:  45\n",
      "*********************\n",
      "Episode:\t 46\n",
      "Reward:\t -130.0\n",
      "Loss:\t 13.018326\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 47\n",
      "Reward:\t -130.0\n",
      "Loss:\t 1.8642948\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 48\n",
      "Reward:\t -125.0\n",
      "Loss:\t 4.6993113\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 49\n",
      "Reward:\t -125.0\n",
      "Loss:\t 2.532302\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 50\n",
      "Reward:\t -125.0\n",
      "Loss:\t 10.33357\n",
      "Was Terminal?:\t False\n",
      "Model Saved at episode:  50\n",
      "*********************\n",
      "Episode:\t 51\n",
      "Reward:\t -125.0\n",
      "Loss:\t 18.223236\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 52\n",
      "Reward:\t -125.0\n",
      "Loss:\t 5.4545097\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 53\n",
      "Reward:\t -125.0\n",
      "Loss:\t 4.2301197\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 54\n",
      "Reward:\t 16.0\n",
      "Loss:\t 18.020573\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 55\n",
      "Reward:\t 40.0\n",
      "Loss:\t 3.066217\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  55\n",
      "*********************\n",
      "Episode:\t 56\n",
      "Reward:\t 94.0\n",
      "Loss:\t 7.4072003\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 57\n",
      "Reward:\t 92.0\n",
      "Loss:\t 4.4260454\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 58\n",
      "Reward:\t 71.0\n",
      "Loss:\t 6.0583334\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 59\n",
      "Reward:\t 71.0\n",
      "Loss:\t 10.618732\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 60\n",
      "Reward:\t -125.0\n",
      "Loss:\t 7.0345507\n",
      "Was Terminal?:\t False\n",
      "Model Saved at episode:  60\n",
      "*********************\n",
      "Episode:\t 61\n",
      "Reward:\t -120.0\n",
      "Loss:\t 18.292778\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 62\n",
      "Reward:\t 51.0\n",
      "Loss:\t 11.102246\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 63\n",
      "Reward:\t 90.0\n",
      "Loss:\t 6.382534\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 64\n",
      "Reward:\t 32.0\n",
      "Loss:\t 4.2493825\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 65\n",
      "Reward:\t -125.0\n",
      "Loss:\t 5.1370993\n",
      "Was Terminal?:\t False\n",
      "Model Saved at episode:  65\n",
      "*********************\n",
      "Episode:\t 66\n",
      "Reward:\t 1.0\n",
      "Loss:\t 4.9511313\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 67\n",
      "Reward:\t -125.0\n",
      "Loss:\t 4.860634\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 68\n",
      "Reward:\t 93.0\n",
      "Loss:\t 4.2926946\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 69\n",
      "Reward:\t 92.0\n",
      "Loss:\t 4.649575\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 70\n",
      "Reward:\t -9.0\n",
      "Loss:\t 6.965664\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  70\n",
      "*********************\n",
      "Episode:\t 71\n",
      "Reward:\t -130.0\n",
      "Loss:\t 3.2648249\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 72\n",
      "Reward:\t -1.0\n",
      "Loss:\t 6.4562798\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 73\n",
      "Reward:\t -130.0\n",
      "Loss:\t 3.7970395\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 74\n",
      "Reward:\t -125.0\n",
      "Loss:\t 5.950053\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 75\n",
      "Reward:\t 30.0\n",
      "Loss:\t 5.9435096\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  75\n",
      "*********************\n",
      "Episode:\t 76\n",
      "Reward:\t 13.0\n",
      "Loss:\t 5.213946\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 77\n",
      "Reward:\t 76.0\n",
      "Loss:\t 6.0094166\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 78\n",
      "Reward:\t 70.0\n",
      "Loss:\t 9.432809\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 79\n",
      "Reward:\t 17.0\n",
      "Loss:\t 11.040218\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 80\n",
      "Reward:\t 43.0\n",
      "Loss:\t 12.264872\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  80\n",
      "*********************\n",
      "Episode:\t 81\n",
      "Reward:\t 75.0\n",
      "Loss:\t 11.714609\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 82\n",
      "Reward:\t 71.0\n",
      "Loss:\t 7.4836144\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 83\n",
      "Reward:\t 42.0\n",
      "Loss:\t 12.80854\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 84\n",
      "Reward:\t 42.0\n",
      "Loss:\t 11.937691\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 85\n",
      "Reward:\t -120.0\n",
      "Loss:\t 4.4531116\n",
      "Was Terminal?:\t False\n",
      "Model Saved at episode:  85\n",
      "*********************\n",
      "Episode:\t 86\n",
      "Reward:\t -120.0\n",
      "Loss:\t 9.535725\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 87\n",
      "Reward:\t 95.0\n",
      "Loss:\t 6.566186\n",
      "Was Terminal?:\t True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************\n",
      "Episode:\t 88\n",
      "Reward:\t 45.0\n",
      "Loss:\t 6.272789\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 89\n",
      "Reward:\t -125.0\n",
      "Loss:\t 4.676659\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 90\n",
      "Reward:\t 64.0\n",
      "Loss:\t 4.4888396\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  90\n",
      "*********************\n",
      "Episode:\t 91\n",
      "Reward:\t 19.0\n",
      "Loss:\t 10.02096\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 92\n",
      "Reward:\t 82.0\n",
      "Loss:\t 9.397436\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 93\n",
      "Reward:\t 49.0\n",
      "Loss:\t 8.630248\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 94\n",
      "Reward:\t 95.0\n",
      "Loss:\t 5.0180902\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 95\n",
      "Reward:\t -130.0\n",
      "Loss:\t 7.970933\n",
      "Was Terminal?:\t False\n",
      "Model Saved at episode:  95\n",
      "*********************\n",
      "Episode:\t 96\n",
      "Reward:\t 16.0\n",
      "Loss:\t 11.76259\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 97\n",
      "Reward:\t 69.0\n",
      "Loss:\t 7.7393007\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 98\n",
      "Reward:\t 43.0\n",
      "Loss:\t 6.1060724\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 99\n",
      "Reward:\t 64.0\n",
      "Loss:\t 12.92108\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 100\n",
      "Reward:\t 95.0\n",
      "Loss:\t 3.802302\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  100\n",
      "*********************\n",
      "Episode:\t 101\n",
      "Reward:\t 61.0\n",
      "Loss:\t 10.336037\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 102\n",
      "Reward:\t -125.0\n",
      "Loss:\t 4.3263197\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 103\n",
      "Reward:\t 45.0\n",
      "Loss:\t 8.035\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 104\n",
      "Reward:\t -18.0\n",
      "Loss:\t 9.116301\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 105\n",
      "Reward:\t 47.0\n",
      "Loss:\t 8.085613\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  105\n",
      "*********************\n",
      "Episode:\t 106\n",
      "Reward:\t 13.0\n",
      "Loss:\t 8.284065\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 107\n",
      "Reward:\t 49.0\n",
      "Loss:\t 6.585193\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 108\n",
      "Reward:\t 47.0\n",
      "Loss:\t 7.5659313\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 109\n",
      "Reward:\t 18.0\n",
      "Loss:\t 5.656356\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 110\n",
      "Reward:\t 59.0\n",
      "Loss:\t 6.1598244\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  110\n",
      "*********************\n",
      "Episode:\t 111\n",
      "Reward:\t 43.0\n",
      "Loss:\t 6.779781\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 112\n",
      "Reward:\t 50.0\n",
      "Loss:\t 4.6280074\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 113\n",
      "Reward:\t -120.0\n",
      "Loss:\t 3.696255\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 114\n",
      "Reward:\t 41.0\n",
      "Loss:\t 5.093618\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 115\n",
      "Reward:\t 46.0\n",
      "Loss:\t 6.737565\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  115\n",
      "*********************\n",
      "Episode:\t 116\n",
      "Reward:\t 41.0\n",
      "Loss:\t 20.526495\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 117\n",
      "Reward:\t 63.0\n",
      "Loss:\t 10.083754\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 118\n",
      "Reward:\t -10.0\n",
      "Loss:\t 6.236285\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 119\n",
      "Reward:\t 41.0\n",
      "Loss:\t 11.858249\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 120\n",
      "Reward:\t 93.0\n",
      "Loss:\t 32.36962\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  120\n",
      "*********************\n",
      "Episode:\t 121\n",
      "Reward:\t 43.0\n",
      "Loss:\t 9.386979\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 122\n",
      "Reward:\t -125.0\n",
      "Loss:\t 5.725014\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 123\n",
      "Reward:\t 14.0\n",
      "Loss:\t 4.7808537\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 124\n",
      "Reward:\t 72.0\n",
      "Loss:\t 4.4431314\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 125\n",
      "Reward:\t 95.0\n",
      "Loss:\t 6.28969\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  125\n",
      "*********************\n",
      "Episode:\t 126\n",
      "Reward:\t 43.0\n",
      "Loss:\t 5.250799\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 127\n",
      "Reward:\t 14.0\n",
      "Loss:\t 8.116757\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 128\n",
      "Reward:\t -10.0\n",
      "Loss:\t 13.350784\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 129\n",
      "Reward:\t 61.0\n",
      "Loss:\t 7.9975004\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 130\n",
      "Reward:\t 89.0\n",
      "Loss:\t 5.0741863\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  130\n",
      "*********************\n",
      "Episode:\t 131\n",
      "Reward:\t 66.0\n",
      "Loss:\t 3.5406723\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 132\n",
      "Reward:\t 95.0\n",
      "Loss:\t 15.755127\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 133\n",
      "Reward:\t 52.0\n",
      "Loss:\t 27.796965\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 134\n",
      "Reward:\t 94.0\n",
      "Loss:\t 13.73082\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 135\n",
      "Reward:\t -125.0\n",
      "Loss:\t 4.8083577\n",
      "Was Terminal?:\t False\n",
      "Model Saved at episode:  135\n",
      "*********************\n",
      "Episode:\t 136\n",
      "Reward:\t 95.0\n",
      "Loss:\t 6.8682814\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 137\n",
      "Reward:\t 95.0\n",
      "Loss:\t 53.954384\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 138\n",
      "Reward:\t 22.0\n",
      "Loss:\t 16.097712\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 139\n",
      "Reward:\t 75.0\n",
      "Loss:\t 15.338239\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 140\n",
      "Reward:\t 95.0\n",
      "Loss:\t 6.5416164\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  140\n",
      "*********************\n",
      "Episode:\t 141\n",
      "Reward:\t 32.0\n",
      "Loss:\t 8.635975\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 142\n",
      "Reward:\t 22.0\n",
      "Loss:\t 12.316776\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 143\n",
      "Reward:\t 19.0\n",
      "Loss:\t 5.475171\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 144\n",
      "Reward:\t 92.0\n",
      "Loss:\t 9.436962\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 145\n",
      "Reward:\t 25.0\n",
      "Loss:\t 7.005222\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  145\n",
      "*********************\n",
      "Episode:\t 146\n",
      "Reward:\t 37.0\n",
      "Loss:\t 7.8362713\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 147\n",
      "Reward:\t 54.0\n",
      "Loss:\t 13.043665\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 148\n",
      "Reward:\t -125.0\n",
      "Loss:\t 5.9469705\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 149\n",
      "Reward:\t 73.0\n",
      "Loss:\t 4.5200844\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 150\n",
      "Reward:\t 95.0\n",
      "Loss:\t 6.9062614\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  150\n",
      "*********************\n",
      "Episode:\t 151\n",
      "Reward:\t 46.0\n",
      "Loss:\t 10.111737\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 152\n",
      "Reward:\t 52.0\n",
      "Loss:\t 13.628018\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 153\n",
      "Reward:\t 76.0\n",
      "Loss:\t 15.603419\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 154\n",
      "Reward:\t -130.0\n",
      "Loss:\t 13.818646\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 155\n",
      "Reward:\t 68.0\n",
      "Loss:\t 8.637335\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  155\n",
      "*********************\n",
      "Episode:\t 156\n",
      "Reward:\t -120.0\n",
      "Loss:\t 5.2012024\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 157\n",
      "Reward:\t 67.0\n",
      "Loss:\t 11.6418\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 158\n",
      "Reward:\t 95.0\n",
      "Loss:\t 8.346063\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 159\n",
      "Reward:\t 49.0\n",
      "Loss:\t 9.280655\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 160\n",
      "Reward:\t 76.0\n",
      "Loss:\t 9.168446\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  160\n",
      "*********************\n",
      "Episode:\t 161\n",
      "Reward:\t 76.0\n",
      "Loss:\t 10.889759\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 162\n",
      "Reward:\t 52.0\n",
      "Loss:\t 9.707483\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 163\n",
      "Reward:\t 83.0\n",
      "Loss:\t 5.0458446\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 164\n",
      "Reward:\t 93.0\n",
      "Loss:\t 6.620421\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 165\n",
      "Reward:\t 31.0\n",
      "Loss:\t 6.163044\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  165\n",
      "*********************\n",
      "Episode:\t 166\n",
      "Reward:\t 37.0\n",
      "Loss:\t 9.678402\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 167\n",
      "Reward:\t 56.0\n",
      "Loss:\t 8.8394165\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 168\n",
      "Reward:\t -120.0\n",
      "Loss:\t 6.311044\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 169\n",
      "Reward:\t -10.0\n",
      "Loss:\t 4.440144\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 170\n",
      "Reward:\t 56.0\n",
      "Loss:\t 8.026097\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  170\n",
      "*********************\n",
      "Episode:\t 171\n",
      "Reward:\t 18.0\n",
      "Loss:\t 6.5390286\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 172\n",
      "Reward:\t 65.0\n",
      "Loss:\t 10.674955\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 173\n",
      "Reward:\t 89.0\n",
      "Loss:\t 30.693941\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 174\n",
      "Reward:\t 86.0\n",
      "Loss:\t 7.7665553\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 175\n",
      "Reward:\t 67.0\n",
      "Loss:\t 5.5375376\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************\n",
      "Episode:\t 176\n",
      "Reward:\t 75.0\n",
      "Loss:\t 12.20301\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 177\n",
      "Reward:\t -5.0\n",
      "Loss:\t 5.5080523\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 178\n",
      "Reward:\t 74.0\n",
      "Loss:\t 9.825525\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 179\n",
      "Reward:\t 72.0\n",
      "Loss:\t 6.888454\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 180\n",
      "Reward:\t 42.0\n",
      "Loss:\t 7.8049517\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  180\n",
      "*********************\n",
      "Episode:\t 181\n",
      "Reward:\t -120.0\n",
      "Loss:\t 5.2430797\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 182\n",
      "Reward:\t 35.0\n",
      "Loss:\t 3.6714854\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 183\n",
      "Reward:\t 56.0\n",
      "Loss:\t 8.539335\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 184\n",
      "Reward:\t 92.0\n",
      "Loss:\t 4.1609454\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 185\n",
      "Reward:\t 34.0\n",
      "Loss:\t 9.071602\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  185\n",
      "*********************\n",
      "Episode:\t 186\n",
      "Reward:\t 71.0\n",
      "Loss:\t 7.8670435\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 187\n",
      "Reward:\t 11.0\n",
      "Loss:\t 7.775685\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 188\n",
      "Reward:\t 65.0\n",
      "Loss:\t 5.4688115\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 189\n",
      "Reward:\t 93.0\n",
      "Loss:\t 38.864235\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 190\n",
      "Reward:\t 51.0\n",
      "Loss:\t 5.758077\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  190\n",
      "*********************\n",
      "Episode:\t 191\n",
      "Reward:\t 60.0\n",
      "Loss:\t 5.2608314\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 192\n",
      "Reward:\t 74.0\n",
      "Loss:\t 4.5207863\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 193\n",
      "Reward:\t 60.0\n",
      "Loss:\t 19.18401\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 194\n",
      "Reward:\t 93.0\n",
      "Loss:\t 6.5729074\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 195\n",
      "Reward:\t 70.0\n",
      "Loss:\t 5.7181234\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  195\n",
      "*********************\n",
      "Episode:\t 196\n",
      "Reward:\t 92.0\n",
      "Loss:\t 15.660728\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 197\n",
      "Reward:\t -120.0\n",
      "Loss:\t 8.697031\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 198\n",
      "Reward:\t 57.0\n",
      "Loss:\t 11.026217\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 199\n",
      "Reward:\t 90.0\n",
      "Loss:\t 12.971246\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 200\n",
      "Reward:\t 41.0\n",
      "Loss:\t 9.588039\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  200\n",
      "*********************\n",
      "Episode:\t 201\n",
      "Reward:\t 47.0\n",
      "Loss:\t 9.221722\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 202\n",
      "Reward:\t 57.0\n",
      "Loss:\t 7.7529325\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 203\n",
      "Reward:\t 30.0\n",
      "Loss:\t 11.609479\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 204\n",
      "Reward:\t 51.0\n",
      "Loss:\t 4.8882446\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 205\n",
      "Reward:\t 36.0\n",
      "Loss:\t 5.525707\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  205\n",
      "*********************\n",
      "Episode:\t 206\n",
      "Reward:\t 50.0\n",
      "Loss:\t 6.580331\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 207\n",
      "Reward:\t 95.0\n",
      "Loss:\t 14.6610985\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 208\n",
      "Reward:\t 92.0\n",
      "Loss:\t 6.1234226\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 209\n",
      "Reward:\t 92.0\n",
      "Loss:\t 15.207725\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 210\n",
      "Reward:\t 46.0\n",
      "Loss:\t 8.217644\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  210\n",
      "*********************\n",
      "Episode:\t 211\n",
      "Reward:\t 65.0\n",
      "Loss:\t 7.930173\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 212\n",
      "Reward:\t 44.0\n",
      "Loss:\t 9.096407\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 213\n",
      "Reward:\t 95.0\n",
      "Loss:\t 6.3786983\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 214\n",
      "Reward:\t 45.0\n",
      "Loss:\t 6.9607143\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 215\n",
      "Reward:\t 38.0\n",
      "Loss:\t 9.779823\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  215\n",
      "*********************\n",
      "Episode:\t 216\n",
      "Reward:\t -120.0\n",
      "Loss:\t 14.851536\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 217\n",
      "Reward:\t 59.0\n",
      "Loss:\t 5.791546\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 218\n",
      "Reward:\t 72.0\n",
      "Loss:\t 12.0921135\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 219\n",
      "Reward:\t 43.0\n",
      "Loss:\t 7.86792\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 220\n",
      "Reward:\t 60.0\n",
      "Loss:\t 6.4653363\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  220\n",
      "*********************\n",
      "Episode:\t 221\n",
      "Reward:\t 93.0\n",
      "Loss:\t 10.123148\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 222\n",
      "Reward:\t 92.0\n",
      "Loss:\t 3.3363955\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 223\n",
      "Reward:\t 54.0\n",
      "Loss:\t 7.1534495\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 224\n",
      "Reward:\t 48.0\n",
      "Loss:\t 56.888454\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 225\n",
      "Reward:\t 95.0\n",
      "Loss:\t 7.8694897\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  225\n",
      "*********************\n",
      "Episode:\t 226\n",
      "Reward:\t -14.0\n",
      "Loss:\t 7.392938\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 227\n",
      "Reward:\t 95.0\n",
      "Loss:\t 7.792302\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 228\n",
      "Reward:\t 68.0\n",
      "Loss:\t 7.7232385\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 229\n",
      "Reward:\t 95.0\n",
      "Loss:\t 6.989828\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 230\n",
      "Reward:\t 94.0\n",
      "Loss:\t 6.9459248\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  230\n",
      "*********************\n",
      "Episode:\t 231\n",
      "Reward:\t 73.0\n",
      "Loss:\t 9.287343\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 232\n",
      "Reward:\t 84.0\n",
      "Loss:\t 6.9434366\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 233\n",
      "Reward:\t 51.0\n",
      "Loss:\t 6.8536434\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 234\n",
      "Reward:\t 93.0\n",
      "Loss:\t 6.724146\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 235\n",
      "Reward:\t 92.0\n",
      "Loss:\t 7.2596073\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  235\n",
      "*********************\n",
      "Episode:\t 236\n",
      "Reward:\t 89.0\n",
      "Loss:\t 4.2092347\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 237\n",
      "Reward:\t 75.0\n",
      "Loss:\t 8.513064\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 238\n",
      "Reward:\t 95.0\n",
      "Loss:\t 7.543664\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 239\n",
      "Reward:\t 50.0\n",
      "Loss:\t 4.1669483\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 240\n",
      "Reward:\t 47.0\n",
      "Loss:\t 10.468422\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  240\n",
      "*********************\n",
      "Episode:\t 241\n",
      "Reward:\t 23.0\n",
      "Loss:\t 3.5753617\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 242\n",
      "Reward:\t 45.0\n",
      "Loss:\t 7.1532087\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 243\n",
      "Reward:\t 64.0\n",
      "Loss:\t 12.90801\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 244\n",
      "Reward:\t -125.0\n",
      "Loss:\t 6.2572994\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 245\n",
      "Reward:\t 94.0\n",
      "Loss:\t 13.091539\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  245\n",
      "*********************\n",
      "Episode:\t 246\n",
      "Reward:\t 61.0\n",
      "Loss:\t 11.468867\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 247\n",
      "Reward:\t 92.0\n",
      "Loss:\t 15.738123\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 248\n",
      "Reward:\t 51.0\n",
      "Loss:\t 14.626413\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 249\n",
      "Reward:\t 38.0\n",
      "Loss:\t 126.20403\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 250\n",
      "Reward:\t 73.0\n",
      "Loss:\t 6.539811\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  250\n",
      "*********************\n",
      "Episode:\t 251\n",
      "Reward:\t 60.0\n",
      "Loss:\t 7.7714267\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 252\n",
      "Reward:\t 75.0\n",
      "Loss:\t 38.261913\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 253\n",
      "Reward:\t 60.0\n",
      "Loss:\t 7.479363\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 254\n",
      "Reward:\t 63.0\n",
      "Loss:\t 8.1338415\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 255\n",
      "Reward:\t 47.0\n",
      "Loss:\t 12.609289\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  255\n",
      "*********************\n",
      "Episode:\t 256\n",
      "Reward:\t 61.0\n",
      "Loss:\t 10.800277\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 257\n",
      "Reward:\t 46.0\n",
      "Loss:\t 6.3418136\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 258\n",
      "Reward:\t 52.0\n",
      "Loss:\t 8.3081\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 259\n",
      "Reward:\t -125.0\n",
      "Loss:\t 7.18434\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 260\n",
      "Reward:\t 85.0\n",
      "Loss:\t 25.02243\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  260\n",
      "*********************\n",
      "Episode:\t 261\n",
      "Reward:\t 95.0\n",
      "Loss:\t 14.328289\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 262\n",
      "Reward:\t -2.0\n",
      "Loss:\t 10.423335\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 263\n",
      "Reward:\t 36.0\n",
      "Loss:\t 6.7807713\n",
      "Was Terminal?:\t True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************\n",
      "Episode:\t 264\n",
      "Reward:\t 63.0\n",
      "Loss:\t 5.209791\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 265\n",
      "Reward:\t 70.0\n",
      "Loss:\t 9.129034\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  265\n",
      "*********************\n",
      "Episode:\t 266\n",
      "Reward:\t 40.0\n",
      "Loss:\t 6.5835257\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 267\n",
      "Reward:\t 94.0\n",
      "Loss:\t 7.9093256\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 268\n",
      "Reward:\t 45.0\n",
      "Loss:\t 6.32007\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 269\n",
      "Reward:\t 68.0\n",
      "Loss:\t 8.069833\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 270\n",
      "Reward:\t 48.0\n",
      "Loss:\t 11.741457\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  270\n",
      "*********************\n",
      "Episode:\t 271\n",
      "Reward:\t 94.0\n",
      "Loss:\t 6.0555935\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 272\n",
      "Reward:\t 51.0\n",
      "Loss:\t 11.460759\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 273\n",
      "Reward:\t -15.0\n",
      "Loss:\t 9.169064\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 274\n",
      "Reward:\t 89.0\n",
      "Loss:\t 8.231043\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 275\n",
      "Reward:\t 74.0\n",
      "Loss:\t 25.875263\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  275\n",
      "*********************\n",
      "Episode:\t 276\n",
      "Reward:\t 70.0\n",
      "Loss:\t 5.3308177\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 277\n",
      "Reward:\t -115.0\n",
      "Loss:\t 7.9026356\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 278\n",
      "Reward:\t -120.0\n",
      "Loss:\t 9.757606\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 279\n",
      "Reward:\t 95.0\n",
      "Loss:\t 4.6851616\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 280\n",
      "Reward:\t 52.0\n",
      "Loss:\t 10.03226\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  280\n",
      "*********************\n",
      "Episode:\t 281\n",
      "Reward:\t 44.0\n",
      "Loss:\t 5.3772764\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 282\n",
      "Reward:\t 80.0\n",
      "Loss:\t 7.998892\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 283\n",
      "Reward:\t -125.0\n",
      "Loss:\t 17.428894\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 284\n",
      "Reward:\t 20.0\n",
      "Loss:\t 7.689412\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 285\n",
      "Reward:\t 24.0\n",
      "Loss:\t 18.602734\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  285\n",
      "*********************\n",
      "Episode:\t 286\n",
      "Reward:\t 64.0\n",
      "Loss:\t 6.5702934\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 287\n",
      "Reward:\t 51.0\n",
      "Loss:\t 8.57412\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 288\n",
      "Reward:\t 57.0\n",
      "Loss:\t 11.439005\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 289\n",
      "Reward:\t 68.0\n",
      "Loss:\t 8.417183\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 290\n",
      "Reward:\t 65.0\n",
      "Loss:\t 18.97017\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  290\n",
      "*********************\n",
      "Episode:\t 291\n",
      "Reward:\t 66.0\n",
      "Loss:\t 7.947642\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 292\n",
      "Reward:\t 43.0\n",
      "Loss:\t 13.253641\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 293\n",
      "Reward:\t 47.0\n",
      "Loss:\t 7.199302\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 294\n",
      "Reward:\t 63.0\n",
      "Loss:\t 3.721173\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 295\n",
      "Reward:\t 61.0\n",
      "Loss:\t 8.6011095\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  295\n",
      "*********************\n",
      "Episode:\t 296\n",
      "Reward:\t 95.0\n",
      "Loss:\t 11.410681\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 297\n",
      "Reward:\t -125.0\n",
      "Loss:\t 6.2871737\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 298\n",
      "Reward:\t 66.0\n",
      "Loss:\t 5.8386555\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 299\n",
      "Reward:\t 64.0\n",
      "Loss:\t 6.868824\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 300\n",
      "Reward:\t 69.0\n",
      "Loss:\t 10.891947\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  300\n",
      "*********************\n",
      "Episode:\t 301\n",
      "Reward:\t 87.0\n",
      "Loss:\t 11.562136\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 302\n",
      "Reward:\t 62.0\n",
      "Loss:\t 5.4182615\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 303\n",
      "Reward:\t 40.0\n",
      "Loss:\t 10.056574\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 304\n",
      "Reward:\t 84.0\n",
      "Loss:\t 13.096392\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 305\n",
      "Reward:\t 89.0\n",
      "Loss:\t 5.4304743\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  305\n",
      "*********************\n",
      "Episode:\t 306\n",
      "Reward:\t 73.0\n",
      "Loss:\t 9.298554\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 307\n",
      "Reward:\t 58.0\n",
      "Loss:\t 10.76568\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 308\n",
      "Reward:\t 58.0\n",
      "Loss:\t 5.0891743\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 309\n",
      "Reward:\t -120.0\n",
      "Loss:\t 9.917102\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 310\n",
      "Reward:\t 90.0\n",
      "Loss:\t 9.895319\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  310\n",
      "*********************\n",
      "Episode:\t 311\n",
      "Reward:\t 65.0\n",
      "Loss:\t 7.789837\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 312\n",
      "Reward:\t 91.0\n",
      "Loss:\t 16.799404\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 313\n",
      "Reward:\t 54.0\n",
      "Loss:\t 14.684702\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 314\n",
      "Reward:\t 36.0\n",
      "Loss:\t 6.012366\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 315\n",
      "Reward:\t 53.0\n",
      "Loss:\t 8.098089\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  315\n",
      "*********************\n",
      "Episode:\t 316\n",
      "Reward:\t 49.0\n",
      "Loss:\t 10.450672\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 317\n",
      "Reward:\t 62.0\n",
      "Loss:\t 6.3372397\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 318\n",
      "Reward:\t 45.0\n",
      "Loss:\t 7.2389936\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 319\n",
      "Reward:\t 63.0\n",
      "Loss:\t 5.557383\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 320\n",
      "Reward:\t 88.0\n",
      "Loss:\t 9.909734\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  320\n",
      "*********************\n",
      "Episode:\t 321\n",
      "Reward:\t -125.0\n",
      "Loss:\t 6.3685274\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 322\n",
      "Reward:\t 38.0\n",
      "Loss:\t 3.898944\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 323\n",
      "Reward:\t 55.0\n",
      "Loss:\t 3.4462814\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 324\n",
      "Reward:\t 66.0\n",
      "Loss:\t 5.358667\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 325\n",
      "Reward:\t 83.0\n",
      "Loss:\t 12.899927\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  325\n",
      "*********************\n",
      "Episode:\t 326\n",
      "Reward:\t 18.0\n",
      "Loss:\t 12.637723\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 327\n",
      "Reward:\t 95.0\n",
      "Loss:\t 10.564825\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 328\n",
      "Reward:\t 51.0\n",
      "Loss:\t 4.792123\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 329\n",
      "Reward:\t 29.0\n",
      "Loss:\t 5.4890366\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 330\n",
      "Reward:\t 85.0\n",
      "Loss:\t 6.6011634\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  330\n",
      "*********************\n",
      "Episode:\t 331\n",
      "Reward:\t 44.0\n",
      "Loss:\t 6.4505982\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 332\n",
      "Reward:\t 67.0\n",
      "Loss:\t 12.098937\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 333\n",
      "Reward:\t 57.0\n",
      "Loss:\t 11.126106\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 334\n",
      "Reward:\t 95.0\n",
      "Loss:\t 5.009573\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 335\n",
      "Reward:\t 95.0\n",
      "Loss:\t 11.232716\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  335\n",
      "*********************\n",
      "Episode:\t 336\n",
      "Reward:\t 93.0\n",
      "Loss:\t 7.6870346\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 337\n",
      "Reward:\t 73.0\n",
      "Loss:\t 4.872985\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 338\n",
      "Reward:\t 60.0\n",
      "Loss:\t 6.2146616\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 339\n",
      "Reward:\t 33.0\n",
      "Loss:\t 7.7017894\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 340\n",
      "Reward:\t 70.0\n",
      "Loss:\t 5.030506\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  340\n",
      "*********************\n",
      "Episode:\t 341\n",
      "Reward:\t 95.0\n",
      "Loss:\t 5.7012897\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 342\n",
      "Reward:\t 58.0\n",
      "Loss:\t 14.540174\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 343\n",
      "Reward:\t 26.0\n",
      "Loss:\t 4.1791363\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 344\n",
      "Reward:\t 94.0\n",
      "Loss:\t 3.635322\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 345\n",
      "Reward:\t 92.0\n",
      "Loss:\t 76.732285\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  345\n",
      "*********************\n",
      "Episode:\t 346\n",
      "Reward:\t 95.0\n",
      "Loss:\t 7.4117584\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 347\n",
      "Reward:\t 64.0\n",
      "Loss:\t 6.3139367\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 348\n",
      "Reward:\t 86.0\n",
      "Loss:\t 6.6147065\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 349\n",
      "Reward:\t 95.0\n",
      "Loss:\t 11.490976\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 350\n",
      "Reward:\t 74.0\n",
      "Loss:\t 9.198841\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  350\n",
      "*********************\n",
      "Episode:\t 351\n",
      "Reward:\t -120.0\n",
      "Loss:\t 9.11115\n",
      "Was Terminal?:\t False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************\n",
      "Episode:\t 352\n",
      "Reward:\t 91.0\n",
      "Loss:\t 8.645686\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 353\n",
      "Reward:\t 88.0\n",
      "Loss:\t 6.669792\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 354\n",
      "Reward:\t 95.0\n",
      "Loss:\t 7.4213486\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 355\n",
      "Reward:\t 72.0\n",
      "Loss:\t 10.090168\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  355\n",
      "*********************\n",
      "Episode:\t 356\n",
      "Reward:\t 95.0\n",
      "Loss:\t 5.011202\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 357\n",
      "Reward:\t 75.0\n",
      "Loss:\t 5.2492704\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 358\n",
      "Reward:\t 67.0\n",
      "Loss:\t 7.346737\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 359\n",
      "Reward:\t 84.0\n",
      "Loss:\t 5.194503\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 360\n",
      "Reward:\t -115.0\n",
      "Loss:\t 14.905788\n",
      "Was Terminal?:\t False\n",
      "Model Saved at episode:  360\n",
      "*********************\n",
      "Episode:\t 361\n",
      "Reward:\t 40.0\n",
      "Loss:\t 8.283341\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 362\n",
      "Reward:\t -115.0\n",
      "Loss:\t 14.39887\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 363\n",
      "Reward:\t 48.0\n",
      "Loss:\t 10.405128\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 364\n",
      "Reward:\t 49.0\n",
      "Loss:\t 9.691517\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 365\n",
      "Reward:\t 74.0\n",
      "Loss:\t 5.8514743\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  365\n",
      "*********************\n",
      "Episode:\t 366\n",
      "Reward:\t -120.0\n",
      "Loss:\t 7.418183\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 367\n",
      "Reward:\t 58.0\n",
      "Loss:\t 6.1554527\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 368\n",
      "Reward:\t 41.0\n",
      "Loss:\t 9.144919\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 369\n",
      "Reward:\t 49.0\n",
      "Loss:\t 24.68617\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 370\n",
      "Reward:\t 92.0\n",
      "Loss:\t 8.178288\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  370\n",
      "*********************\n",
      "Episode:\t 371\n",
      "Reward:\t 54.0\n",
      "Loss:\t 8.042894\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 372\n",
      "Reward:\t 68.0\n",
      "Loss:\t 3.572753\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 373\n",
      "Reward:\t 70.0\n",
      "Loss:\t 11.955242\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 374\n",
      "Reward:\t 95.0\n",
      "Loss:\t 6.5864425\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 375\n",
      "Reward:\t 63.0\n",
      "Loss:\t 6.0820646\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  375\n",
      "*********************\n",
      "Episode:\t 376\n",
      "Reward:\t 95.0\n",
      "Loss:\t 4.014898\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 377\n",
      "Reward:\t 95.0\n",
      "Loss:\t 7.052684\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 378\n",
      "Reward:\t 65.0\n",
      "Loss:\t 28.476355\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 379\n",
      "Reward:\t 52.0\n",
      "Loss:\t 10.658685\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 380\n",
      "Reward:\t 95.0\n",
      "Loss:\t 7.700573\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  380\n",
      "*********************\n",
      "Episode:\t 381\n",
      "Reward:\t 67.0\n",
      "Loss:\t 7.7352424\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 382\n",
      "Reward:\t 65.0\n",
      "Loss:\t 5.288409\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 383\n",
      "Reward:\t -125.0\n",
      "Loss:\t 7.0414104\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 384\n",
      "Reward:\t 83.0\n",
      "Loss:\t 11.627981\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 385\n",
      "Reward:\t 65.0\n",
      "Loss:\t 4.674564\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  385\n",
      "*********************\n",
      "Episode:\t 386\n",
      "Reward:\t 89.0\n",
      "Loss:\t 7.3441873\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 387\n",
      "Reward:\t 92.0\n",
      "Loss:\t 9.990091\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 388\n",
      "Reward:\t 67.0\n",
      "Loss:\t 5.7634387\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 389\n",
      "Reward:\t 80.0\n",
      "Loss:\t 8.075581\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 390\n",
      "Reward:\t 77.0\n",
      "Loss:\t 5.6683874\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  390\n",
      "*********************\n",
      "Episode:\t 391\n",
      "Reward:\t 87.0\n",
      "Loss:\t 7.571005\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 392\n",
      "Reward:\t 93.0\n",
      "Loss:\t 11.180494\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 393\n",
      "Reward:\t 57.0\n",
      "Loss:\t 6.599443\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 394\n",
      "Reward:\t 92.0\n",
      "Loss:\t 16.271688\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 395\n",
      "Reward:\t 56.0\n",
      "Loss:\t 4.9747095\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  395\n",
      "*********************\n",
      "Episode:\t 396\n",
      "Reward:\t 95.0\n",
      "Loss:\t 3.940743\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 397\n",
      "Reward:\t 85.0\n",
      "Loss:\t 3.9086154\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 398\n",
      "Reward:\t 56.0\n",
      "Loss:\t 5.988566\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 399\n",
      "Reward:\t 95.0\n",
      "Loss:\t 17.196106\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 400\n",
      "Reward:\t 58.0\n",
      "Loss:\t 10.066811\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  400\n",
      "*********************\n",
      "Episode:\t 401\n",
      "Reward:\t 59.0\n",
      "Loss:\t 9.767328\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 402\n",
      "Reward:\t 71.0\n",
      "Loss:\t 7.819216\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 403\n",
      "Reward:\t -120.0\n",
      "Loss:\t 8.066929\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 404\n",
      "Reward:\t 57.0\n",
      "Loss:\t 10.46698\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 405\n",
      "Reward:\t 58.0\n",
      "Loss:\t 19.40316\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  405\n",
      "*********************\n",
      "Episode:\t 406\n",
      "Reward:\t 50.0\n",
      "Loss:\t 5.3280096\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 407\n",
      "Reward:\t 63.0\n",
      "Loss:\t 9.713629\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 408\n",
      "Reward:\t 36.0\n",
      "Loss:\t 5.059857\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 409\n",
      "Reward:\t 85.0\n",
      "Loss:\t 15.468214\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 410\n",
      "Reward:\t 65.0\n",
      "Loss:\t 7.429383\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  410\n",
      "*********************\n",
      "Episode:\t 411\n",
      "Reward:\t 61.0\n",
      "Loss:\t 9.107487\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 412\n",
      "Reward:\t 69.0\n",
      "Loss:\t 4.517296\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 413\n",
      "Reward:\t 95.0\n",
      "Loss:\t 8.750586\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 414\n",
      "Reward:\t -115.0\n",
      "Loss:\t 6.177205\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 415\n",
      "Reward:\t 71.0\n",
      "Loss:\t 6.98623\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  415\n",
      "*********************\n",
      "Episode:\t 416\n",
      "Reward:\t 95.0\n",
      "Loss:\t 7.45968\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 417\n",
      "Reward:\t -120.0\n",
      "Loss:\t 14.511885\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 418\n",
      "Reward:\t 49.0\n",
      "Loss:\t 5.037257\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 419\n",
      "Reward:\t 52.0\n",
      "Loss:\t 6.4824133\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 420\n",
      "Reward:\t 92.0\n",
      "Loss:\t 7.377562\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  420\n",
      "*********************\n",
      "Episode:\t 421\n",
      "Reward:\t 75.0\n",
      "Loss:\t 5.245555\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 422\n",
      "Reward:\t 71.0\n",
      "Loss:\t 16.762333\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 423\n",
      "Reward:\t 45.0\n",
      "Loss:\t 4.7675076\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 424\n",
      "Reward:\t 49.0\n",
      "Loss:\t 6.2357106\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 425\n",
      "Reward:\t 84.0\n",
      "Loss:\t 4.91295\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  425\n",
      "*********************\n",
      "Episode:\t 426\n",
      "Reward:\t 61.0\n",
      "Loss:\t 6.827147\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 427\n",
      "Reward:\t 72.0\n",
      "Loss:\t 6.9022026\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 428\n",
      "Reward:\t 87.0\n",
      "Loss:\t 8.827623\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 429\n",
      "Reward:\t 74.0\n",
      "Loss:\t 8.012175\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 430\n",
      "Reward:\t 82.0\n",
      "Loss:\t 6.9336557\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  430\n",
      "*********************\n",
      "Episode:\t 431\n",
      "Reward:\t 62.0\n",
      "Loss:\t 6.9015684\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 432\n",
      "Reward:\t 60.0\n",
      "Loss:\t 7.0629044\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 433\n",
      "Reward:\t 58.0\n",
      "Loss:\t 8.96233\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 434\n",
      "Reward:\t 95.0\n",
      "Loss:\t 4.666378\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 435\n",
      "Reward:\t -115.0\n",
      "Loss:\t 14.396621\n",
      "Was Terminal?:\t False\n",
      "Model Saved at episode:  435\n",
      "*********************\n",
      "Episode:\t 436\n",
      "Reward:\t 86.0\n",
      "Loss:\t 9.559071\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 437\n",
      "Reward:\t 91.0\n",
      "Loss:\t 7.2595463\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 438\n",
      "Reward:\t 95.0\n",
      "Loss:\t 6.0068617\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 439\n",
      "Reward:\t 82.0\n",
      "Loss:\t 11.899709\n",
      "Was Terminal?:\t True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************\n",
      "Episode:\t 440\n",
      "Reward:\t 73.0\n",
      "Loss:\t 10.131065\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  440\n",
      "*********************\n",
      "Episode:\t 441\n",
      "Reward:\t 61.0\n",
      "Loss:\t 6.751134\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 442\n",
      "Reward:\t 83.0\n",
      "Loss:\t 13.970741\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 443\n",
      "Reward:\t 95.0\n",
      "Loss:\t 18.10119\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 444\n",
      "Reward:\t 57.0\n",
      "Loss:\t 7.979479\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 445\n",
      "Reward:\t 75.0\n",
      "Loss:\t 16.02927\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  445\n",
      "*********************\n",
      "Episode:\t 446\n",
      "Reward:\t 42.0\n",
      "Loss:\t 7.6240463\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 447\n",
      "Reward:\t 76.0\n",
      "Loss:\t 5.1810274\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 448\n",
      "Reward:\t -120.0\n",
      "Loss:\t 9.038743\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 449\n",
      "Reward:\t 48.0\n",
      "Loss:\t 6.015636\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 450\n",
      "Reward:\t 80.0\n",
      "Loss:\t 10.107715\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  450\n",
      "*********************\n",
      "Episode:\t 451\n",
      "Reward:\t 49.0\n",
      "Loss:\t 6.94977\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 452\n",
      "Reward:\t 65.0\n",
      "Loss:\t 6.632331\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 453\n",
      "Reward:\t -120.0\n",
      "Loss:\t 10.104437\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 454\n",
      "Reward:\t 89.0\n",
      "Loss:\t 7.9434605\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 455\n",
      "Reward:\t 63.0\n",
      "Loss:\t 9.860668\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  455\n",
      "*********************\n",
      "Episode:\t 456\n",
      "Reward:\t 61.0\n",
      "Loss:\t 7.494578\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 457\n",
      "Reward:\t 95.0\n",
      "Loss:\t 6.084566\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 458\n",
      "Reward:\t 95.0\n",
      "Loss:\t 11.17071\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 459\n",
      "Reward:\t 95.0\n",
      "Loss:\t 5.780257\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 460\n",
      "Reward:\t 56.0\n",
      "Loss:\t 4.570512\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  460\n",
      "*********************\n",
      "Episode:\t 461\n",
      "Reward:\t 57.0\n",
      "Loss:\t 8.918093\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 462\n",
      "Reward:\t 71.0\n",
      "Loss:\t 8.538993\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 463\n",
      "Reward:\t 92.0\n",
      "Loss:\t 12.778492\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 464\n",
      "Reward:\t 54.0\n",
      "Loss:\t 5.97488\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 465\n",
      "Reward:\t 95.0\n",
      "Loss:\t 3.1419525\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  465\n",
      "*********************\n",
      "Episode:\t 466\n",
      "Reward:\t 79.0\n",
      "Loss:\t 7.15142\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 467\n",
      "Reward:\t 95.0\n",
      "Loss:\t 5.948968\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 468\n",
      "Reward:\t 68.0\n",
      "Loss:\t 12.845299\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 469\n",
      "Reward:\t 60.0\n",
      "Loss:\t 3.2565808\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 470\n",
      "Reward:\t 48.0\n",
      "Loss:\t 4.7061596\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  470\n",
      "*********************\n",
      "Episode:\t 471\n",
      "Reward:\t 61.0\n",
      "Loss:\t 7.5048633\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 472\n",
      "Reward:\t 49.0\n",
      "Loss:\t 5.2813435\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 473\n",
      "Reward:\t 83.0\n",
      "Loss:\t 20.91687\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 474\n",
      "Reward:\t 63.0\n",
      "Loss:\t 5.7912436\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 475\n",
      "Reward:\t -120.0\n",
      "Loss:\t 10.61244\n",
      "Was Terminal?:\t False\n",
      "Model Saved at episode:  475\n",
      "*********************\n",
      "Episode:\t 476\n",
      "Reward:\t 85.0\n",
      "Loss:\t 8.476908\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 477\n",
      "Reward:\t 68.0\n",
      "Loss:\t 4.4992065\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 478\n",
      "Reward:\t 65.0\n",
      "Loss:\t 6.3971543\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 479\n",
      "Reward:\t 70.0\n",
      "Loss:\t 7.9160523\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 480\n",
      "Reward:\t 66.0\n",
      "Loss:\t 7.249507\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  480\n",
      "*********************\n",
      "Episode:\t 481\n",
      "Reward:\t 95.0\n",
      "Loss:\t 5.585634\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 482\n",
      "Reward:\t 89.0\n",
      "Loss:\t 7.9294043\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 483\n",
      "Reward:\t -110.0\n",
      "Loss:\t 7.535583\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 484\n",
      "Reward:\t 95.0\n",
      "Loss:\t 5.878761\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 485\n",
      "Reward:\t 71.0\n",
      "Loss:\t 5.421921\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  485\n",
      "*********************\n",
      "Episode:\t 486\n",
      "Reward:\t 64.0\n",
      "Loss:\t 12.268436\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 487\n",
      "Reward:\t -115.0\n",
      "Loss:\t 5.6233377\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 488\n",
      "Reward:\t 66.0\n",
      "Loss:\t 8.029571\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 489\n",
      "Reward:\t 45.0\n",
      "Loss:\t 6.6996207\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 490\n",
      "Reward:\t 59.0\n",
      "Loss:\t 5.880106\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  490\n",
      "*********************\n",
      "Episode:\t 491\n",
      "Reward:\t 70.0\n",
      "Loss:\t 4.641037\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 492\n",
      "Reward:\t -115.0\n",
      "Loss:\t 8.741676\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 493\n",
      "Reward:\t 89.0\n",
      "Loss:\t 4.2094383\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 494\n",
      "Reward:\t -120.0\n",
      "Loss:\t 6.894038\n",
      "Was Terminal?:\t False\n",
      "*********************\n",
      "Episode:\t 495\n",
      "Reward:\t 62.0\n",
      "Loss:\t 3.8260143\n",
      "Was Terminal?:\t True\n",
      "Model Saved at episode:  495\n",
      "*********************\n",
      "Episode:\t 496\n",
      "Reward:\t 65.0\n",
      "Loss:\t 3.391418\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 497\n",
      "Reward:\t 58.0\n",
      "Loss:\t 3.563609\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 498\n",
      "Reward:\t 68.0\n",
      "Loss:\t 5.1979\n",
      "Was Terminal?:\t True\n",
      "*********************\n",
      "Episode:\t 499\n",
      "Reward:\t 61.0\n",
      "Loss:\t 8.416576\n",
      "Was Terminal?:\t True\n"
     ]
    }
   ],
   "source": [
    "#training loop\n",
    "if training: \n",
    "    with tf.Session() as sess:\n",
    "        #init vars and decay step\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        decay_step = 0 \n",
    "        \n",
    "        #rewards for each episode \n",
    "        episodic_rewards = [] \n",
    "        \n",
    "        #init doom  \n",
    "        game.init()\n",
    "        \n",
    "        for episode in range(total_episodes):\n",
    "            #init steps for episode \n",
    "            step = 0 \n",
    "            \n",
    "            #init observation\n",
    "            game.init()\n",
    "            game.new_episode()\n",
    "            state = game.get_state().screen_buffer\n",
    "            state, stacked_frame = stack_frames(stacked_frames, state, True)\n",
    "            \n",
    "            #list for rewards collected in episode \n",
    "            rewards_list = []\n",
    "            \n",
    "            while step < max_steps:\n",
    "                ################SAMPLING#######################    \n",
    "                #increments \n",
    "                step += 1\n",
    "                decay_step += 1\n",
    "                #if step % 50 == 0: print (step)\n",
    "                #choose action \n",
    "                action, explore_prob = explore_exploit(epsilon, min_eps, decay_rate, decay_step, state, num_actions)\n",
    "\n",
    "                #take action                 \n",
    "                reward = game.make_action(action)\n",
    "                \n",
    "                #check if game is done \n",
    "                done = game.is_episode_finished()\n",
    "                \n",
    "                rewards_list.append(reward) \n",
    "                \n",
    "                #if were not dead \n",
    "                if not done:\n",
    "                    #get next state\n",
    "                    next_state = game.get_state().screen_buffer\n",
    "                    \n",
    "                    #store transition after converting to proper format \n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "                    #add memory \n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                    \n",
    "                    #update state\n",
    "                    state = next_state\n",
    "\n",
    "                #if were dead     \n",
    "                else: \n",
    "                    #update next state \n",
    "                    next_state = np.zeros((84,84), dtype=np.int)\n",
    "\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    \n",
    "                    #add episodic rewards \n",
    "                    tot_reward = np.sum(rewards_list)\n",
    "                    episodic_rewards.append((episode, tot_reward))\n",
    "                    \n",
    "                    #end episode \n",
    "                    step = max_steps\n",
    "                    \n",
    "                    #add exp \n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "                ################Learning#######################    \n",
    "                #extract minibatch values \n",
    "                mini_batch = memory.sample(bs)\n",
    "                states_batch = np.array([sample[0] for sample in mini_batch])\n",
    "                actions_batch = np.array([sample[1] for sample in mini_batch])\n",
    "                rewards_batch = np.array([sample[2] for sample in mini_batch])\n",
    "                next_states_batch = np.array([sample[3] for sample in mini_batch])\n",
    "                dones_batch = np.array([sample[4] for sample in mini_batch])\n",
    "                \n",
    "                #target qs to be set below \n",
    "                targ_qs_batch = []\n",
    "                \n",
    "                #get predicted Q's for each next state \n",
    "                next_qs_batch = sess.run(DQNetwork.out, feed_dict={DQNetwork.inputs : next_states_batch})\n",
    "                \n",
    "                #determine if state is terminal and set value for target_q\n",
    "                for i in range(bs):\n",
    "                    final_state = dones_batch[i]\n",
    "                    \n",
    "                    if final_state:\n",
    "                        targ_qs_batch.append(rewards_batch[i])\n",
    "                    \n",
    "                    else:   \n",
    "                        target_q = rewards_batch[i] + gamma * np.max(next_qs_batch[i])\n",
    "                        targ_qs_batch.append(target_q)\n",
    "                    \n",
    "                \n",
    "                #convert to np array \n",
    "                target_q = np.array([i for i in targ_qs_batch])\n",
    "                \n",
    "                #determine loss \n",
    "                loss, _ = sess.run([DQNetwork.loss, DQNetwork.optim], feed_dict={DQNetwork.inputs: states_batch,\n",
    "                                                                               DQNetwork.target_Q: targ_qs_batch,\n",
    "                                                                               DQNetwork.actions: actions_batch})\n",
    "                \n",
    "                \n",
    "                #perform gradient update \n",
    "                \n",
    "            #episode summary \n",
    "            print (\"*********************\")\n",
    "            print (\"Episode:\\t\", episode)\n",
    "            print (\"Reward:\\t\", np.sum(rewards_list))\n",
    "            print (\"Loss:\\t\", loss)\n",
    "            print (\"Was Terminal?:\\t\", done)\n",
    "\n",
    "            if episode % 5 == 0:\n",
    "                save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "                print(\"Model Saved at episode: \", episode)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/raed/Projects/rl-implementations/DQN'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/model.ckpt\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 83.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 68.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 55.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 47.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 77.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 52.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 95.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 85.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 54.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 67.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 95.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: -350.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 89.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 95.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 84.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 83.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 57.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 55.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 89.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 68.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 66.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 56.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 63.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 66.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 86.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 65.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 89.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 95.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: -345.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 63.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 80.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 78.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 95.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 95.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 95.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: -355.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 95.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 95.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 95.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 86.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 55.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 58.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 64.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 89.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: -345.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 64.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 89.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 95.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 95.0\n",
      ">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                    Total Reward: 76.0\n",
      "****************************\n",
      "Average Reward over 50 episodes:\t 43.02\n"
     ]
    }
   ],
   "source": [
    "test_episodes = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    #setup env \n",
    "    game, possible_actions = init_env()\n",
    "    \n",
    "    #load the model \n",
    "    saver.restore(sess, \"./models/model.ckpt\")\n",
    "    game.init()\n",
    "    total_score = 0 \n",
    "\n",
    "    #run for each episode  \n",
    "    for episode in range(test_episodes):\n",
    "             \n",
    "        game.new_episode()\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "        while not game.is_episode_finished():\n",
    "            #choose action corresponding to best q val \n",
    "            q_preds = sess.run(DQNetwork.out, feed_dict={DQNetwork.inputs:state.reshape((1,*state.shape))})            \n",
    "            choice = np.argmax(q_preds)\n",
    "            action = possible_actions[int(choice)]\n",
    "            \n",
    "            #take action\n",
    "            game.make_action(action)\n",
    "            done = game.is_episode_finished()\n",
    "            score = game.get_total_reward()\n",
    "            \n",
    "            if done:  #if game is done  \n",
    "                break \n",
    "            \n",
    "            else:\n",
    "                next_state = game.get_state().screen_buffer\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                state = next_state \n",
    "            \n",
    "        print (\"\"\">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
    "                    Total Reward: {}\"\"\".format(game.get_total_reward())) \n",
    "        total_score += score\n",
    "    \n",
    "                 \n",
    "    game.close()\n",
    "    print (\"****************************\")\n",
    "    print (\"Average Reward over {} episodes:\\t\".format(test_episodes), total_score/float(test_episodes))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
