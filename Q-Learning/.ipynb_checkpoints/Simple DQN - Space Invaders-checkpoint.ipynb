{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import retro \n",
    "\n",
    "from skimage import transform \n",
    "from skimage.color import rgb2gray \n",
    "\n",
    "from collections import deque \n",
    "\n",
    "import random \n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Cannot create multiple emulator instances per process",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d277b7aed9c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'SpaceInvaders-Atari2600'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/retro/__init__.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(game, state, inttype, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Game not found: %s. Did you make sure to import the ROM?'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mRetroEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minttype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minttype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/retro/retro_env.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, game, state, scenario, info, use_restricted_actions, record, players, inttype)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# emulator, ensure that unused ones are garbage-collected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRetroEmulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrom_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot create multiple emulator instances per process"
     ]
    }
   ],
   "source": [
    "env = retro.make(game='SpaceInvaders-Atari2600')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Size: \t Box(210, 160, 3)\n",
      "Action Size: \t 8\n",
      "[[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "frame_size = env.observation_space \n",
    "num_actions = env.action_space.n\n",
    "\n",
    "print (\"Frame Size: \\t\", frame_size)\n",
    "print(\"Action Size: \\t\", num_actions)\n",
    "\n",
    "#one hot encoded actions matrix \n",
    "actions_1_hot = np.array(np.identity(num_actions, dtype=int).tolist())\n",
    "print(actions_1_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    \"\"\"\n",
    "    Convert frames to grayscale, crop irrelevent parts, normalize pixels, \n",
    "    resize frame.\n",
    "    \n",
    "    Input is frame of size 210x160x3\n",
    "    Returns frame of size 110x84x1 \n",
    "    \"\"\"\n",
    "    grayed = rgb2gray(frame)\n",
    "    cropped = grayed[8:-12, 4:-12]\n",
    "    normed = cropped/255.0 \n",
    "    resized = transform.resize(normed, [110,84])\n",
    "    \n",
    "    return resized #returns preprocessed frame \n",
    "\n",
    "stack_size = 4\n",
    "def stack_frames(stacked_frames, frame, is_new_episode, stack_size=stack_size):\n",
    "    \"\"\"\n",
    "    takes a frame/state and preprocesses it, \n",
    "    if same episode:\n",
    "        adds to the stack state \n",
    "    else new episode: \n",
    "        creates stacked state \n",
    "    \n",
    "    where the stacked state is 4 stacked states(frames)\n",
    "    returns stacked state where axis=1 is for different frames \n",
    "    \"\"\"\n",
    "    frame = preprocess_frame(frame)\n",
    "    if not is_new_episode:\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "    \n",
    "    else: \n",
    "        #init deque \n",
    "        stacked_frames = deque([np.zeros((110,84), dtype=np.int) \n",
    "                                for i in range(stack_size)], maxlen=4)\n",
    "        #new episode so same frame x4 \n",
    "        for i in range(4): stacked_frames.append(frame)\n",
    "        \n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "\n",
    "    return stacked_state, stacked_frames\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110, 84, 4) (4, 110, 84)\n"
     ]
    }
   ],
   "source": [
    "#init frame stack\n",
    "stacked_frames  =  deque([np.zeros((110,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "frame = np.zeros((110,84), dtype=np.int)\n",
    "\n",
    "#test helper functions \n",
    "sample_state, stacked_frames = stack_frames(stacked_frames, frame, True)\n",
    "print(np.shape(sample_state), np.shape(stacked_frames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env params \n",
    "total_episodes = 50 #total num of episodes \n",
    "total_test_episodes = 10 #total num of episodes to test on \n",
    "max_steps = 50000 #max num of steps per episode \n",
    "bs = 64\n",
    "\n",
    "#network params \n",
    "lr = 0.00025\n",
    "state_size = [110,84,4]\n",
    "\n",
    "#discount factor \n",
    "gamma = 0.9\n",
    "\n",
    "#exploration params \n",
    "epsilon = 1.0 #starting value for eps greedy (explore)\n",
    "max_eps = 1.0  #max value for eps greey \n",
    "min_eps = 0.01 #min value for eps greedy \n",
    "decay = 0.00001 #decay rate for eps \n",
    "\n",
    "#Recall Params\n",
    "memory_size = 1000000\n",
    "pretrained_len = bs #number of init memories \n",
    "\n",
    "training=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN: Model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='DQN', training=training):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.lr = learning_rate\n",
    "        self.training = training\n",
    "        with tf.variable_scope(name):\n",
    "            #placeholders\n",
    "            self.inputs = tf.placeholder(tf.float32,[None, *self.state_size])\n",
    "            self.actions = tf.placeholder(tf.float32, [None, self.action_size])\n",
    "            self.target_Q = tf.placeholder(tf.float32)\n",
    "            \n",
    "            #block 1 \n",
    "            self.conv1 = tf.layers.conv2d(inputs=self.inputs, filters=32, \n",
    "                                          kernel_size=[8,8], \n",
    "                                          strides=(4,4), \n",
    "                                          padding='valid', \n",
    "                                          kernel_initializer= tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            self.bn1 = tf.layers.batch_normalization(self.conv1, epsilon=1e-5, training=self.training)\n",
    "            self.elu1 = tf.nn.elu(self.bn1)\n",
    "            \n",
    "            #block 2\n",
    "            self.conv2 = tf.layers.conv2d(inputs=self.elu1, filters=64, \n",
    "                                          kernel_size=[4,4], \n",
    "                                          strides=(2,2), \n",
    "                                          padding='valid', \n",
    "                                          kernel_initializer= tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            self.bn2 = tf.layers.batch_normalization(self.conv2, epsilon=1e-5, training=self.training)\n",
    "            self.elu2 = tf.nn.elu(self.bn2)\n",
    "            \n",
    "            #block 3\n",
    "            self.conv3 = tf.layers.conv2d(inputs=self.elu2, filters=128, \n",
    "                                          kernel_size=[4,4], \n",
    "                                          strides=(2,2), \n",
    "                                          padding='valid', \n",
    "                                          kernel_initializer= tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            self.bn3 = tf.layers.batch_normalization(self.conv3, epsilon=1e-5, training=self.training)\n",
    "            self.elu3 = tf.nn.elu(self.bn3)\n",
    "            \n",
    "            #FC Block \n",
    "            self.flat = tf.layers.flatten(self.elu3)\n",
    "            self.fc = tf.layers.dense(self.flat, units=512, activation=tf.nn.elu, \n",
    "                                       kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.out = tf.layers.dense(self.fc, units=self.action_size, activation=None, \n",
    "                                       kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            #Q-Value prediction \n",
    "            self.pred_Q = tf.reduce_sum(tf.multiply(self.out, self.actions), axis=1)\n",
    "            \n",
    "            #Loss Function \n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_Q-self.pred_Q))\n",
    "            \n",
    "            #Optimizer \n",
    "            self.optim = tf.train.RMSPropOptimizer(learning_rate=self.lr).minimize(self.loss)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#init \n",
    "DQNetwork = DQN(state_size, num_actions, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#memory class creates and manages deque \n",
    "\n",
    "class Memory():\n",
    "    def __init__(self, mem_limit):\n",
    "        self.cache = deque(maxlen=mem_limit)\n",
    "    \n",
    "    def add(self, exp):\n",
    "        self.cache.append(exp)\n",
    "    \n",
    "    def sample(self, sample_size):\n",
    "        sampling_ind = np.random.choice(np.arange(len(self.cache)), size=sample_size, replace=False)\n",
    "        return [self.cache[i] for i in sampling_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init memory for first time \n",
    "memory = Memory(mem_limit=memory_size)\n",
    "\n",
    "import pdb \n",
    "\n",
    "for i in range(pretrained_len):\n",
    "    #pdb.set_trace()\n",
    "    #initilize state for first step  \n",
    "    if i == 0: \n",
    "        state = env.reset() #init state \n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "    #take a random action, record observations and add if not dead \n",
    "    choice = np.random.choice(np.arange(num_actions),size=1, replace=False)\n",
    "    action = actions_1_hot[choice][0]    \n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    \n",
    "    #comment out if you want unseen \n",
    "    env.render()\n",
    "    \n",
    "    #stack new frame \n",
    "    next_state, stacked_frames = stack_frames(stacked_frames, obs, False)\n",
    "    \n",
    "    if not done:\n",
    "        #add memory \n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        #update state\n",
    "        state = next_state\n",
    "        \n",
    "    else: #were dead\n",
    "        #update next state \n",
    "        next_state = np.zeros_like(state)\n",
    "        \n",
    "        #add exp \n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "        #new episode and restart \n",
    "        env.reset()\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to predict action based on epsilon greedy \n",
    "\n",
    "def explore_exploit(epsilon, min_eps, decay_rate, decay_step, state, num_actions):\n",
    "    #produce random number between 0 and 1 \n",
    "    check = np.random.rand()\n",
    "    explore_prob = min_eps + (epsilon-min_eps)*np.exp(-decay_rate*decay_step)\n",
    "    \n",
    "    #explore\n",
    "    if explore_prob>check:\n",
    "        #take random action \n",
    "        choice = random.randint(1,len(actions_1_hot))-1\n",
    "        action = actions_1_hot[choice]  \n",
    "        \n",
    "    #exploit \n",
    "    else:\n",
    "        #q vals predicted by network \n",
    "        Qs = sess.run(DQNetwork.out, feed_dict = {DQNetwork.inputs : state.reshape((1,*state.shape))})\n",
    "        #choose action corresponding to best Q value \n",
    "        choice = np.argmax(Qs)\n",
    "        action = actions_1_hot[choice]\n",
    "    return action, explore_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-8e262374baec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;31m#get predicted Q's for each next state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0mnext_qs_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDQNetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mDQNetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mnext_states_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0;31m#determine if state is terminal and set value for target_q\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#for saving \n",
    "saver = tf.train.Saver()\n",
    "training = True \n",
    "episode_render = True \n",
    "decay_rate = decay\n",
    "\n",
    "if training == True: \n",
    "    with tf.Session() as sess:\n",
    "        #init vars and decay rate\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        decay_step = 0 \n",
    "        \n",
    "        #rewards for each episode \n",
    "        episodic_rewards = [] \n",
    "        \n",
    "        for episode in range(total_episodes):\n",
    "            #init steps for episode \n",
    "            step = 0 \n",
    "            \n",
    "            #init observation\n",
    "            state = env.reset()\n",
    "            state, stacked_frame = stack_frames(stacked_frames, state, True)\n",
    "            \n",
    "            #list for rewards collected in episode \n",
    "            rewards_list = []\n",
    "            \n",
    "            while step < max_steps:\n",
    "                ################SAMPLING#######################    \n",
    "                #increments \n",
    "                step += 1\n",
    "                decay_step += 1\n",
    "                #if step % 50 == 0: print (step)\n",
    "                #choose action \n",
    "                action, explore_prob = explore_exploit(epsilon, min_eps, decay_rate, decay_step, state, num_actions)\n",
    "\n",
    "                #take action                 \n",
    "                next_state, reward, done, _ = env.step(action)    \n",
    "                if episode_render: env.render()\n",
    "                \n",
    "                rewards_list.append(reward) \n",
    "                \n",
    "                #if were not dead \n",
    "                if not done:\n",
    "                    #store transition after converting to proper format \n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, obs, False)\n",
    "\n",
    "                    #add memory \n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                    #update state\n",
    "                    state = next_state\n",
    "\n",
    "                #if were dead     \n",
    "                else: \n",
    "                    #update next state \n",
    "                    next_state = np.zeros_like(state)\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    \n",
    "                    #add episodic rewards \n",
    "                    tot_reward = np.sum(rewards_list)\n",
    "                    episodic_rewards.append((episode, tot_reward))\n",
    "                    \n",
    "                    #print episode summary \n",
    "                    print(  'Episode: {}'.format(episode),\n",
    "                            'Total reward: {}'.format(tot_reward),\n",
    "                            'Explore P: {:.4f}'.format(explore_prob),\n",
    "                            'Training Loss {:.4f}'.format(loss))\n",
    "                    \n",
    "                    #end episode \n",
    "                    step = max_steps\n",
    "                    \n",
    "                    #add exp \n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "                ################Learning#######################    \n",
    "                #extract minibatch values \n",
    "                mini_batch = memory.sample(bs)\n",
    "                states_batch = [sample[0] for sample in mini_batch]\n",
    "                actions_batch = [sample[1] for sample in mini_batch]\n",
    "                rewards_batch = [sample[2] for sample in mini_batch]\n",
    "                next_states_batch = [sample[3] for sample in mini_batch]\n",
    "                dones_batch = [sample[4] for sample in mini_batch]\n",
    "                \n",
    "                #target qs to be set below \n",
    "                targ_qs_batch = []\n",
    "                \n",
    "                #get predicted Q's for each next state \n",
    "                next_qs_batch = sess.run(DQNetwork.out, feed_dict={DQNetwork.inputs : next_states_batch})\n",
    "                \n",
    "                #determine if state is terminal and set value for target_q\n",
    "                for i in range(bs):\n",
    "                    final_state = dones_batch[i]\n",
    "                    \n",
    "                    if final_state:\n",
    "                        targ_qs_batch.append(rewards_batch[i])\n",
    "                    \n",
    "                    else:   \n",
    "                        target_q = rewards_batch[i] + gamma * np.max(next_qs_batch[i])\n",
    "                        targ_qs_batch.append(target_q)\n",
    "                    \n",
    "                \n",
    "                #convert to np array \n",
    "                target_q = np.array([i for i in targ_qs_batch])\n",
    "                \n",
    "                #determine loss \n",
    "                loss, _ = sess.run([DQNetwork.loss, DQNetwork.optim], feed_dict={DQNetwork.inputs: states_batch,\n",
    "                                                                               DQNetwork.target_Q: targ_qs_batch,\n",
    "                                                                               DQNetwork.actions: actions_batch})\n",
    "                \n",
    "                \n",
    "                #perform gradient update \n",
    "                \n",
    "            if episode % 5 == 0:\n",
    "                save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "                print(\"Model Saved\")              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
