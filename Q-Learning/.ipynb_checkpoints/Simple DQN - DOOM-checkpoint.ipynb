{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raed/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from vizdoom import *\n",
    "\n",
    "\n",
    "from skimage import transform \n",
    "from skimage.color import rgb2gray \n",
    "\n",
    "from collections import deque \n",
    "\n",
    "import time \n",
    "import random \n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_env():\n",
    "    game = DoomGame()\n",
    "    game.load_config(\"basic.cfg\")\n",
    "    game.set_doom_scenario_path(\"basic.wad\")\n",
    "    game.init()\n",
    "    \n",
    "    left = [1,0,0]\n",
    "    shoot = [0,0,1]\n",
    "    right = [0,1,0]\n",
    "    \n",
    "    possible_actions = [left, right, shoot]\n",
    "    \n",
    "    return game, possible_actions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_env(test_eps = 10):\n",
    "    \"\"\"\n",
    "    Perform random action and test env to ensure it works \n",
    "    \"\"\"\n",
    "    game = DoomGame()\n",
    "    game.load_config(\"basic.cfg\")\n",
    "    game.set_doom_scenario_path(\"basic.wad\")\n",
    "    game.init()\n",
    "    \n",
    "    left = [1,0,0]\n",
    "    shoot = [0,0,1]\n",
    "    right = [0,1,0]\n",
    "    \n",
    "    possible_actions = [left, right, shoot]\n",
    "    \n",
    "    for i in range(test_eps):\n",
    "        game.new_episode()\n",
    "        while not game.is_episode_finished():\n",
    "            state = game.get_state()\n",
    "            img = state.screen_buffer\n",
    "            misc = state.game_variables\n",
    "            action = random.choice(possible_actions)\n",
    "            print (action)\n",
    "            \n",
    "            reward = game.make_action(action)\n",
    "            print (\"\\treward:\", reward)\n",
    "            \n",
    "            time.sleep(0.02)\n",
    "        \n",
    "        #prints every episode \n",
    "        print (\"Result:\", game.get_total_reward())\n",
    "        time.sleep(2)\n",
    "    \n",
    "    game.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -6.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -6.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -6.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -6.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -6.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -6.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -6.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -6.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -6.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -6.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -6.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -6.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -6.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -6.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -6.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -6.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "Result: -380.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -6.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -6.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -6.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -6.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -6.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -6.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -6.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -6.0\n",
      "[0, 0, 1]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[1, 0, 0]\n",
      "\treward: -1.0\n",
      "[0, 1, 0]\n"
     ]
    },
    {
     "ename": "ViZDoomUnexpectedExitException",
     "evalue": "Controlled ViZDoom instance exited unexpectedly.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mViZDoomUnexpectedExitException\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-073c2055f666>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-584cfb77459d>\u001b[0m in \u001b[0;36mtest_env\u001b[0;34m(test_eps)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"\\treward:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mViZDoomUnexpectedExitException\u001b[0m: Controlled ViZDoom instance exited unexpectedly."
     ]
    }
   ],
   "source": [
    "test_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call init_env \n",
    "game, possible_actions = init_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    \"\"\"\n",
    "    Convert frames to grayscale, crop irrelevent parts, normalize pixels, \n",
    "    resize frame.\n",
    "    \n",
    "    Input is frame of size 210x160x3\n",
    "    Returns frame of size 84x84x1 \n",
    "    \"\"\"\n",
    "    #graying done by vizdoom\n",
    "    #grayed = rgb2gray(frame)\n",
    "    \n",
    "    cropped = frame[30:-10, 30:-30]\n",
    "    normed = cropped/255.0 \n",
    "    resized = transform.resize(normed, [84,84])\n",
    "    \n",
    "    return resized #returns preprocessed frame \n",
    "\n",
    "stack_size = 4\n",
    "def stack_frames(stacked_frames, frame, is_new_episode, stack_size=stack_size):\n",
    "    \"\"\"\n",
    "    takes a frame/state and preprocesses it, \n",
    "    if same episode:\n",
    "        adds to the stack state \n",
    "    else new episode: \n",
    "        creates stacked state \n",
    "    \n",
    "    where the stacked state is 4 stacked states(frames)\n",
    "    returns stacked state where axis=1 is for different frames \n",
    "    \"\"\"\n",
    "    frame = preprocess_frame(frame)\n",
    "    \n",
    "    if not is_new_episode:\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "    \n",
    "    else: \n",
    "        #init deque \n",
    "        stacked_frames = deque([np.zeros((84,84), dtype=np.int) \n",
    "                                for i in range(stack_size)], maxlen=4)\n",
    "        #new episode so same frame x4 \n",
    "        for i in range(4): stacked_frames.append(frame)\n",
    "        \n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "\n",
    "    return stacked_state, stacked_frames\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 84, 4) (4, 84, 84)\n"
     ]
    }
   ],
   "source": [
    "#init frame stack\n",
    "stacked_frames  =  deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "frame = np.zeros((84,84), dtype=np.int)\n",
    "\n",
    "#test helper functions \n",
    "sample_state, stacked_frames = stack_frames(stacked_frames, frame, True)\n",
    "print(np.shape(sample_state), np.shape(stacked_frames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training params  \n",
    "total_episodes = 500 #total num of episodes \n",
    "total_test_episodes = 10 #total num of episodes to test on \n",
    "max_steps = 100 #max num of steps per episode \n",
    "bs = 64\n",
    "\n",
    "#network params \n",
    "lr = 0.0002 #learning rate \n",
    "state_size = [84,84,4]  #4 84x84 frames \n",
    "action_size = game.get_available_buttons_size() # 3 actions\n",
    "\n",
    "#discount factor \n",
    "gamma = 0.95 \n",
    "\n",
    "#exploration params \n",
    "epsilon = 1.0 #starting value for eps greedy (explore)\n",
    "max_eps = 1.0  #max value for eps greey \n",
    "min_eps = 0.01 #min value for eps greedy \n",
    "decay_rate = 0.0001 #decay rate for eps \n",
    "\n",
    "#Recall Params\n",
    "memory_size = 1000000\n",
    "pretrained_len = bs #number of init memories \n",
    "\n",
    "#Training Mode \n",
    "training=True \n",
    "\n",
    "#Env should be rendered or not \n",
    "episode_render = True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN: Model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='DQN', training=training):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.lr = learning_rate\n",
    "        self.training = training\n",
    "        with tf.variable_scope(name):\n",
    "            #placeholders\n",
    "            self.inputs = tf.placeholder(tf.float32,[None, *self.state_size])\n",
    "            self.actions = tf.placeholder(tf.float32, [None, self.action_size])\n",
    "            self.target_Q = tf.placeholder(tf.float32)\n",
    "            \n",
    "            #block 1 \n",
    "            self.conv1 = tf.layers.conv2d(inputs=self.inputs, filters=32, \n",
    "                                          kernel_size=[8,8], \n",
    "                                          strides=(4,4), \n",
    "                                          padding='valid', \n",
    "                                          kernel_initializer= tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            self.bn1 = tf.layers.batch_normalization(self.conv1, epsilon=1e-5, training=self.training)\n",
    "            self.elu1 = tf.nn.elu(self.bn1)\n",
    "            \n",
    "            #block 2\n",
    "            self.conv2 = tf.layers.conv2d(inputs=self.elu1, filters=64, \n",
    "                                          kernel_size=[4,4], \n",
    "                                          strides=(2,2), \n",
    "                                          padding='valid', \n",
    "                                          kernel_initializer= tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            self.bn2 = tf.layers.batch_normalization(self.conv2, epsilon=1e-5, training=self.training)\n",
    "            self.elu2 = tf.nn.elu(self.bn2)\n",
    "            \n",
    "            #block 3\n",
    "            self.conv3 = tf.layers.conv2d(inputs=self.elu2, filters=128, \n",
    "                                          kernel_size=[4,4], \n",
    "                                          strides=(2,2), \n",
    "                                          padding='valid', \n",
    "                                          kernel_initializer= tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            self.bn3 = tf.layers.batch_normalization(self.conv3, epsilon=1e-5, training=self.training)\n",
    "            self.elu3 = tf.nn.elu(self.bn3)\n",
    "            \n",
    "            #FC Block \n",
    "            self.flat = tf.layers.flatten(self.elu3)\n",
    "            self.fc = tf.layers.dense(self.flat, units=512, activation=tf.nn.elu, \n",
    "                                       kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.out = tf.layers.dense(self.fc, units=self.action_size, activation=None, \n",
    "                                       kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            #Q-Value prediction \n",
    "            self.pred_Q = tf.reduce_sum(tf.multiply(self.out, self.actions), axis=1)\n",
    "            \n",
    "            #Loss Function \n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_Q-self.pred_Q))\n",
    "            \n",
    "            #Optimizer \n",
    "            self.optim = tf.train.RMSPropOptimizer(learning_rate=self.lr).minimize(self.loss)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#init\n",
    "num_actions = len(possible_actions)\n",
    "DQNetwork = DQN(state_size, num_actions, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#memory class creates and manages deque \n",
    "\n",
    "class Memory():\n",
    "    def __init__(self, mem_limit):\n",
    "        self.cache = deque(maxlen=mem_limit)\n",
    "    \n",
    "    def add(self, exp):\n",
    "        self.cache.append(exp)\n",
    "    \n",
    "    def sample(self, sample_size):\n",
    "        sampling_ind = np.random.choice(np.arange(len(self.cache)), size=sample_size, replace=False)\n",
    "        return [self.cache[i] for i in sampling_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#activate and populate memory \n",
    "\n",
    "memory = Memory(mem_limit=memory_size)\n",
    "\n",
    "import pdb \n",
    "\n",
    "game.new_episode()\n",
    "\n",
    "for i in range(pretrained_len):\n",
    "    #pdb.set_trace()\n",
    "    #initilize state for first step  \n",
    "    if i == 0: \n",
    "        state = game.get_state().screen_buffer #init state \n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "    #take a random action\n",
    "    action = random.choice(possible_actions)\n",
    "    \n",
    "    #reward from chosen action \n",
    "    reward = game.make_action(action)\n",
    "    \n",
    "    #check if done \n",
    "    done = game.is_episode_finished()\n",
    "    \n",
    "    if not done:\n",
    "        #get next state \n",
    "        next_state = game.get_state().screen_buffer\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "        #add memory \n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        #update state\n",
    "        state = next_state\n",
    "        \n",
    "    else: #were dead\n",
    "        #update next state \n",
    "        next_state = np.zeros((84,84), dtype=np.int)\n",
    "\n",
    "        #add exp \n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "        #new episode and restart \n",
    "        game.new_episode()\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to predict action based on epsilon greedy \n",
    "\n",
    "def explore_exploit(epsilon, min_eps, decay_rate, decay_step, state, num_actions):\n",
    "    #produce random number between 0 and 1 \n",
    "    check = np.random.rand()\n",
    "    explore_prob = min_eps + (epsilon-min_eps)*np.exp(-decay_rate*decay_step)\n",
    "    \n",
    "    #explore\n",
    "    if explore_prob>check:\n",
    "        #take random action \n",
    "        action = random.choice(possible_actions)  \n",
    "        \n",
    "    #exploit \n",
    "    else:\n",
    "        #q vals predicted by network \n",
    "        Qs = sess.run(DQNetwork.out, feed_dict = {DQNetwork.inputs : state.reshape((1,*state.shape))})\n",
    "        #choose action corresponding to best Q value \n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[int(choice)]\n",
    "    return action, explore_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for saving \n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileDoesNotExistException",
     "evalue": "File \"basic.wad\" does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileDoesNotExistException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-7e923651f719>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#init doom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileDoesNotExistException\u001b[0m: File \"basic.wad\" does not exist."
     ]
    }
   ],
   "source": [
    "#training loop\n",
    "if training: \n",
    "    with tf.Session() as sess:\n",
    "        #init vars and decay step\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        decay_step = 0 \n",
    "        \n",
    "        #rewards for each episode \n",
    "        episodic_rewards = [] \n",
    "        \n",
    "        #init doom  \n",
    "        game.init()\n",
    "        \n",
    "        for episode in range(total_episodes):\n",
    "            #init steps for episode \n",
    "            step = 0 \n",
    "            \n",
    "            #init observation\n",
    "            game.new_episode()\n",
    "            state = game.get_state().screen_buffer\n",
    "            state, stacked_frame = stack_frames(stacked_frames, state, True)\n",
    "            \n",
    "            #list for rewards collected in episode \n",
    "            rewards_list = []\n",
    "            \n",
    "            while step < max_steps:\n",
    "                ################SAMPLING#######################    \n",
    "                #increments \n",
    "                step += 1\n",
    "                decay_step += 1\n",
    "                #if step % 50 == 0: print (step)\n",
    "                #choose action \n",
    "                action, explore_prob = explore_exploit(epsilon, min_eps, decay_rate, decay_step, state, num_actions)\n",
    "\n",
    "                #take action                 \n",
    "                reward = game.make_action(action)\n",
    "                \n",
    "                #check if game is done \n",
    "                done = game.is_episode_finished()\n",
    "                \n",
    "                rewards_list.append(reward) \n",
    "                \n",
    "                #if were not dead \n",
    "                if not done:\n",
    "                    #get next state\n",
    "                    next_state = game.get_state().screen_buffer\n",
    "                    \n",
    "                    #store transition after converting to proper format \n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "                    #add memory \n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                    \n",
    "                    #update state\n",
    "                    state = next_state\n",
    "\n",
    "                #if were dead     \n",
    "                else: \n",
    "                    #update next state \n",
    "                    next_state = np.zeros((84,84), dtype=np.int)\n",
    "\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    \n",
    "                    #add episodic rewards \n",
    "                    tot_reward = np.sum(rewards_list)\n",
    "                    episodic_rewards.append((episode, tot_reward))\n",
    "                    \n",
    "                    #print episode summary \n",
    "                    print(  'Episode: {}'.format(episode),\n",
    "                            'Total reward: {}'.format(tot_reward),\n",
    "                            'Explore P: {:.4f}'.format(explore_prob),\n",
    "                            'Training Loss {:.4f}'.format(loss))\n",
    "                    \n",
    "                    #end episode \n",
    "                    step = max_steps\n",
    "                    \n",
    "                    #add exp \n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "                ################Learning#######################    \n",
    "                #extract minibatch values \n",
    "                mini_batch = memory.sample(bs)\n",
    "                states_batch = np.array([sample[0] for sample in mini_batch])\n",
    "                actions_batch = np.array([sample[1] for sample in mini_batch])\n",
    "                rewards_batch = np.array([sample[2] for sample in mini_batch])\n",
    "                next_states_batch = np.array([sample[3] for sample in mini_batch])\n",
    "                dones_batch = np.array([sample[4] for sample in mini_batch])\n",
    "                \n",
    "                #target qs to be set below \n",
    "                targ_qs_batch = []\n",
    "                \n",
    "                #get predicted Q's for each next state \n",
    "                next_qs_batch = sess.run(DQNetwork.out, feed_dict={DQNetwork.inputs : next_states_batch})\n",
    "                \n",
    "                #determine if state is terminal and set value for target_q\n",
    "                for i in range(bs):\n",
    "                    final_state = dones_batch[i]\n",
    "                    \n",
    "                    if final_state:\n",
    "                        targ_qs_batch.append(rewards_batch[i])\n",
    "                    \n",
    "                    else:   \n",
    "                        target_q = rewards_batch[i] + gamma * np.max(next_qs_batch[i])\n",
    "                        targ_qs_batch.append(target_q)\n",
    "                    \n",
    "                \n",
    "                #convert to np array \n",
    "                target_q = np.array([i for i in targ_qs_batch])\n",
    "                \n",
    "                #determine loss \n",
    "                loss, _ = sess.run([DQNetwork.loss, DQNetwork.optim], feed_dict={DQNetwork.inputs: states_batch,\n",
    "                                                                               DQNetwork.target_Q: targ_qs_batch,\n",
    "                                                                               DQNetwork.actions: actions_batch})\n",
    "                \n",
    "                \n",
    "                #perform gradient update \n",
    "                \n",
    "            if episode % 5 == 0:\n",
    "                save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "                print (\"Episode: \\t\", episode)\n",
    "                \n",
    "                print(\"Model Saved\")              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/model.ckpt\n",
      "\n",
      "                >>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
      "                Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "test_episodes = 1 \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    #setup env \n",
    "    game, possible_actions = init_env()\n",
    "    \n",
    "    #load the model \n",
    "    saver.restore(sess, \"./models/model.ckpt\")\n",
    "    game.init()\n",
    "\n",
    "    #run for each episode  \n",
    "    for episode in range(test_episodes):\n",
    "             \n",
    "        game.new_episode()\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "        while not game.is_episode_finished():\n",
    "            #choose action corresponding to best q val \n",
    "            q_preds = sess.run(DQNetwork.out, feed_dict={DQNetwork.inputs:state})            \n",
    "            choice = np.argmax(q_preds)\n",
    "            action = possible_actions[int(choice)]\n",
    "            \n",
    "            #take action\n",
    "            game.make_action(action)\n",
    "            done = game.is_episode_finished()\n",
    "            score = game.get_total_reward()\n",
    "            \n",
    "            if done:  #if game is done \n",
    "                break \n",
    "            \n",
    "            else:\n",
    "                next_state = game.get_state().screen_buffer\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                state = next_state \n",
    "            \n",
    "        print (\"\"\">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
    "                    Total Reward: {}\"\"\".format(game.get_total_reward())) \n",
    "            \n",
    "\n",
    "    #env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
