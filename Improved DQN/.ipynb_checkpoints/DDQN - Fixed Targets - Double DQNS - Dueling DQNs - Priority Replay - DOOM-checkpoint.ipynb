{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raed/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from vizdoom import *\n",
    "\n",
    "\n",
    "from skimage import transform \n",
    "from skimage.color import rgb2gray \n",
    "\n",
    "from collections import deque \n",
    "\n",
    "import time \n",
    "import random \n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_env():\n",
    "    game = DoomGame()\n",
    "    game.load_config(\"basic.cfg\")\n",
    "    game.set_doom_scenario_path(\"basic.wad\")\n",
    "    game.init()\n",
    "    \n",
    "    left = [1,0,0]\n",
    "    shoot = [0,0,1]\n",
    "    right = [0,1,0]\n",
    "    \n",
    "    possible_actions = [left, right, shoot]\n",
    "    \n",
    "    return game, possible_actions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_env(test_eps = 10):\n",
    "    \"\"\"\n",
    "    Perform random action and test env to ensure it works \n",
    "    \"\"\"\n",
    "    game = DoomGame()\n",
    "    game.load_config(\"basic.cfg\")\n",
    "    game.set_doom_scenario_path(\"basic.wad\")\n",
    "    game.init()\n",
    "    \n",
    "    left = [1,0,0]\n",
    "    shoot = [0,0,1]\n",
    "    right = [0,1,0]\n",
    "    \n",
    "    possible_actions = [left, right, shoot]\n",
    "    \n",
    "    for i in range(test_eps):\n",
    "        game.new_episode()\n",
    "        while not game.is_episode_finished():\n",
    "            state = game.get_state()\n",
    "            img = state.screen_buffer\n",
    "            misc = state.game_variables\n",
    "            action = random.choice(possible_actions)\n",
    "            print (action)\n",
    "            \n",
    "            reward = game.make_action(action)\n",
    "            print (\"\\treward:\", reward)\n",
    "            \n",
    "            time.sleep(0.02)\n",
    "        \n",
    "        #prints every episode \n",
    "        print (\"Result:\", game.get_total_reward())\n",
    "        time.sleep(2)\n",
    "    \n",
    "    game.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call init_env \n",
    "game, possible_actions = init_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    \"\"\"\n",
    "    Convert frames to grayscale, crop irrelevent parts, normalize pixels, \n",
    "    resize frame.\n",
    "    \n",
    "    Input is frame of size 210x160x3\n",
    "    Returns frame of size 110x84x1 \n",
    "    \"\"\"\n",
    "    #graying done by vizdoom\n",
    "    #grayed = rgb2gray(frame)\n",
    "    \n",
    "    cropped = frame[30:-10, 30:-30]\n",
    "    normed = cropped/255.0 \n",
    "    resized = transform.resize(normed, [84,84])\n",
    "    \n",
    "    return resized #returns preprocessed frame \n",
    "\n",
    "def stack_frames(stacked_frames, frame, is_new_episode, stack_size=4):\n",
    "    \"\"\"\n",
    "    takes a frame/state and preprocesses it, \n",
    "    if same episode:\n",
    "        adds to the stack state \n",
    "    else new episode: \n",
    "        creates stacked state \n",
    "    \n",
    "    where the stacked state is 4 stacked states(frames)\n",
    "    returns stacked state where axis=1 is for different frames \n",
    "    \"\"\"\n",
    "    frame = preprocess_frame(frame)\n",
    "    \n",
    "    if not is_new_episode:\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "    \n",
    "    else: \n",
    "        #init deque \n",
    "        stacked_frames = deque([np.zeros((110,84), dtype=np.int) \n",
    "                                for i in range(stack_size)], maxlen=4)\n",
    "        #new episode so same frame x4 \n",
    "        for i in range(4): stacked_frames.append(frame)\n",
    "        \n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "\n",
    "    return stacked_state, stacked_frames\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 84, 4) (4, 84, 84)\n"
     ]
    }
   ],
   "source": [
    "stack_size = 4\n",
    "\n",
    "#init frame stack\n",
    "stacked_frames  =  deque([np.zeros((110,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "frame = np.zeros((110,84), dtype=np.int)\n",
    "\n",
    "#test helper functions \n",
    "sample_state, stacked_frames = stack_frames(stacked_frames, frame, True)\n",
    "print(np.shape(sample_state), np.shape(stacked_frames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training params  \n",
    "total_episodes = 5000 #total num of episodes \n",
    "total_test_episodes = 10 #total num of episodes to test on \n",
    "max_steps = 5000 #max num of steps per episode \n",
    "bs = 64\n",
    "\n",
    "#network params \n",
    "lr = 0.0002 #learning rate \n",
    "state_size = [84,84,4]  #4 84x84 frames \n",
    "action_size = game.get_available_buttons_size() # 3 actions\n",
    "\n",
    "#Fixed Targets Params \n",
    "max_tau = 10000\n",
    "\n",
    "#discount factor \n",
    "gamma = 0.95 \n",
    "\n",
    "#exploration params \n",
    "epsilon = 1.0 #starting value for eps greedy (explore)\n",
    "max_eps = 1.0  #max value for eps greey \n",
    "min_eps = 0.01 #min value for eps greedy \n",
    "decay = 0.0001 #decay rate for eps \n",
    "\n",
    "#Recall Params\n",
    "memory_size = 1000000\n",
    "pretrained_len = 1000000 #number of init memories \n",
    "\n",
    "#Training Mode \n",
    "training=True \n",
    "\n",
    "#Env should be rendered or not \n",
    "episode_render = True \n",
    "\n",
    "#Prioritized Experience Sampling \n",
    "PER_e = 0.01  # Hyperparameter that we use to avoid some experiences that have 0 probability of being taken\n",
    "PER_a = 0.6  # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly\n",
    "PER_b = 0.4  # importance-sampling, from initial value increasing to 1    \n",
    "PER_b_increment_per_sampling = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN: Simple Model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='DQN', training=training):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.lr = learning_rate\n",
    "        self.training = training\n",
    "        with tf.variable_scope(name):\n",
    "            #placeholders\n",
    "            self.inputs = tf.placeholder(tf.float32,[None, *self.state_size])\n",
    "            self.actions = tf.placeholder(tf.float32, [None, self.action_size])\n",
    "            self.target_Q = tf.placeholder(tf.float32)\n",
    "            \n",
    "            #block 1 \n",
    "            self.conv1 = tf.layers.conv2d(inputs=self.inputs, filters=32, \n",
    "                                          kernel_size=[8,8], \n",
    "                                          strides=(4,4), \n",
    "                                          padding='valid', \n",
    "                                          kernel_initializer= tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            self.bn1 = tf.layers.batch_normalization(self.conv1, epsilon=1e-5, training=self.training)\n",
    "            self.elu1 = tf.nn.elu(self.bn1)\n",
    "            \n",
    "            #block 2\n",
    "            self.conv2 = tf.layers.conv2d(inputs=self.elu1, filters=64, \n",
    "                                          kernel_size=[4,4], \n",
    "                                          strides=(2,2), \n",
    "                                          padding='valid', \n",
    "                                          kernel_initializer= tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            self.bn2 = tf.layers.batch_normalization(self.conv2, epsilon=1e-5, training=self.training)\n",
    "            self.elu2 = tf.nn.elu(self.bn2)\n",
    "            \n",
    "            #block 3\n",
    "            self.conv3 = tf.layers.conv2d(inputs=self.elu2, filters=128, \n",
    "                                          kernel_size=[4,4], \n",
    "                                          strides=(2,2), \n",
    "                                          padding='valid', \n",
    "                                          kernel_initializer= tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            self.bn3 = tf.layers.batch_normalization(self.conv3, epsilon=1e-5, training=self.training)\n",
    "            self.elu3 = tf.nn.elu(self.bn3)\n",
    "            \n",
    "            #FC Block \n",
    "            self.flat = tf.layers.flatten(self.elu3)\n",
    "            self.fc = tf.layers.dense(self.flat, units=512, activation=tf.nn.elu, \n",
    "                                       kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.out = tf.layers.dense(self.fc, units=self.action_size, activation=None, \n",
    "                                       kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            #Q-Value prediction \n",
    "            self.pred_Q = tf.reduce_sum(tf.multiply(self.out, self.actions), axis=1)\n",
    "            \n",
    "            #Loss Function \n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_Q-self.pred_Q))\n",
    "            \n",
    "            #Optimizer \n",
    "            self.optim = tf.train.RMSPropOptimizer(learning_rate=self.lr).minimize(self.loss)\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dueling DQN \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN ():\n",
    "    def __init__(self, state_size, action_size, learning_rate, name):\n",
    "        self.state_size = state_size \n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.name = name \n",
    "        \n",
    "        #tf.variablescope makes it clear which network were working on \n",
    "        #recall parameter update function would require this \n",
    "        with tf.variable_scope(name):\n",
    "            #placeholders\n",
    "            self.inputs = tf.placeholder(tf.float32,[None, *self.state_size])\n",
    "            self.actions = tf.placeholder(tf.float32, [None, self.action_size])\n",
    "            self.target_Q = tf.placeholder(tf.float32)\n",
    "            \n",
    "            #block 1 \n",
    "            self.conv1 = tf.layers.conv2d(inputs=self.inputs, filters=32, \n",
    "                                          kernel_size=[8,8], \n",
    "                                          strides=(4,4), \n",
    "                                          padding='valid', \n",
    "                                          kernel_initializer= tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            #self.bn1 = tf.layers.batch_normalization(self.conv1, epsilon=1e-5, training=self.training)\n",
    "            self.elu1 = tf.nn.elu(self.conv1)\n",
    "            \n",
    "            #block 2\n",
    "            self.conv2 = tf.layers.conv2d(inputs=self.elu1, filters=64, \n",
    "                                          kernel_size=[4,4], \n",
    "                                          strides=(2,2), \n",
    "                                          padding='valid', \n",
    "                                          kernel_initializer= tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            #self.bn2 = tf.layers.batch_normalization(self.conv2, epsilon=1e-5, training=self.training)\n",
    "            self.elu2 = tf.nn.elu(self.conv2)\n",
    "            \n",
    "            #block 3\n",
    "            self.conv3 = tf.layers.conv2d(inputs=self.elu2, filters=128, \n",
    "                                          kernel_size=[4,4], \n",
    "                                          strides=(2,2), \n",
    "                                          padding='valid', \n",
    "                                          kernel_initializer= tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            #self.bn3 = tf.layers.batch_normalization(self.conv3, epsilon=1e-5, training=self.training)\n",
    "            self.elu3 = tf.nn.elu(self.conv3)\n",
    "            \n",
    "            #flatten \n",
    "            self.flat = tf.layers.flatten(self.elu3)     \n",
    "            \n",
    "            #FC Stream 1: V(s) \n",
    "            self.fc_v = tf.layers.dense(self.flat, units=512, activation=tf.nn.elu, \n",
    "                                       kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.value = tf.layers.dense(self.fc_v, units=1, activation=None, \n",
    "                                       kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            #FC Stream 2 A(s,a)\n",
    "            self.fc_a = tf.layers.dense(self.flat, units=512, activation=tf.nn.elu, \n",
    "                                       kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.advantage = tf.layers.dense(self.fc_a, units=self.action_size, activation=None, \n",
    "                                       kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            \n",
    "            #Q(s,a) estim by aggregation: Q(s,a) = V(s) + (A(s,a) - 1/|A| * sum A(s,a')) \n",
    "            self.out = self.value + tf.subtract(self.advantage, tf.reduce_mean(self.advantage, axis=1, keepdims=True))\n",
    "            \n",
    "            #Q-Value prediction \n",
    "            self.pred_Q = tf.reduce_sum(tf.multiply(self.out, self.actions), axis=1)\n",
    "            \n",
    "            #Loss Function \n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_Q-self.pred_Q))\n",
    "            \n",
    "            #Optimizer \n",
    "            self.optim = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "num_actions = len(possible_actions)\n",
    "\n",
    "#init the two networks \n",
    "#predictive network for updating weights \n",
    "DQNetwork = DDQN(state_size, num_actions, lr, name=\"DQNetwork\")\n",
    "\n",
    "#td target network with more stable weights \n",
    "TargetNetwork = DDQN(state_size, num_actions, lr, name=\"TargetNetwork\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sumtree is binary tree with leaves that contain the prioritiy, the index of a leave \n",
    "#is the corresponding index of an experience in a data array \n",
    "\n",
    "class SumTree():\n",
    "        \n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"\n",
    "        init nodes and data \n",
    "        \"\"\"\n",
    "        #number of experiences (i.e. number of leaf nodes)\n",
    "        self.capacity = capacity\n",
    "        \n",
    "        #init tree, minus 1 for root node \n",
    "        self.tree = np.zeros(2*capacity-1) \n",
    "        \n",
    "        #data for experiences \n",
    "        self.data = np.zeros((capacity,5))\n",
    "    \n",
    "        #data pointer for above array \n",
    "        self.data_index = 0 \n",
    "    \n",
    "    \n",
    "    def add(self, priority, exp):\n",
    "        \"\"\"\n",
    "        add exp Priority score to tree and data \n",
    "        \"\"\"\n",
    "        #determine index for tree \n",
    "        tree_index = self.data_index + self.capacity - 1\n",
    "         \n",
    "        #update data array \n",
    "        self.data[self.data_index,:] = [exp]\n",
    "        \n",
    "        #update leaf \n",
    "        self.update(tree_index, priority)\n",
    "        \n",
    "        #update data pointer by 1 \n",
    "        self.data_index += 1 \n",
    "        \n",
    "        #overwrite if over capacity \n",
    "        if self.data_index >= self.capacity:\n",
    "            self.data_index = 0 \n",
    "\n",
    "    \n",
    "    def update(self, tree_index, priority):\n",
    "        \"\"\"\n",
    "        Update binary sum tree \n",
    "        \"\"\"\n",
    "        #calc change in priority\n",
    "        delta = priority - self.tree[tree_index] \n",
    "        \n",
    "        #update tree \n",
    "        self.tree[tree_index] = priority\n",
    "        \n",
    "        #propogate change up tree \n",
    "        while tree_index != 0: \n",
    "            tree_index = (tree_index - 1) // 2\n",
    "            self.tree[tree_index] += delta\n",
    "    \n",
    "    def get_leaf(self, v):\n",
    "        \"\"\"\n",
    "        Get leaf index, priority value, and the experience within the data array.\n",
    "        Given the priority value.\n",
    "        \"\"\"\n",
    "        parent_index = 0\n",
    "        \n",
    "        while True: \n",
    "            left_child_index = parent_index * 2 + 1 \n",
    "            right_child_index = left_child_index + 1 \n",
    "            \n",
    "            #if we reach end of tree, leaf index must be parent index \n",
    "            if left_child_index > len(self.tree):\n",
    "                leaf_index = parent_index\n",
    "                break \n",
    "            \n",
    "            #otherwise search downwards for the higher priority node \n",
    "            else:\n",
    "                if v<= self.tree[left_child_index]:\n",
    "                    parent_index = left_child_index\n",
    "                else:\n",
    "                    v -= self.tree_index[left_child_index]\n",
    "                    parent_index = right_child_index\n",
    "        \n",
    "        data_index = leaf_index - self.capacity + 1\n",
    "        \n",
    "        return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
    "    \n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#memory class stores (s,a,r,s_) in sumtree \n",
    "\n",
    "class Memory():\n",
    "    \n",
    "    def __init__(self, capacity, abs_err_upper = 0):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.abs_err = abs_err_upper\n",
    "        \n",
    "    def add(self, exp):\n",
    "        #find max priority \n",
    "        max_p = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "        #check to ensure its not zero \n",
    "        if max_p == 0:\n",
    "            max_p = self.abs_err\n",
    "        \n",
    "        #adds new exp with max_p\n",
    "        self.tree.add(max_p, exp)\n",
    "        \n",
    "    def sample(self, n):\n",
    "        #to contain minibatch \n",
    "        memory_batch = []\n",
    "        b_idx, b_ISWeights = np.empty((n,), dtype=np.int32), np.empty((n, 1), dtype=np.float32)\n",
    "\n",
    "        #determine priority ranges \n",
    "        priority_ranges = self.tree.total_priority / n\n",
    "        \n",
    "        # Here we increasing the PER_b each time we sample a new minibatch\n",
    "        PER_b = np.min([1., PER_b + PER_b_increment_per_sampling])  # max = 1\n",
    "        \n",
    "        #max weight \n",
    "        p_min = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.total_priority\n",
    "        max_weight = (p_min * n) ** (-PER_b)\n",
    "        \n",
    "        for i in range(n):\n",
    "            #sample value in range \n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "\n",
    "            #find experience corresponding to value \n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "            \n",
    "            #P(j)\n",
    "            sampling_probabilities = priority / self.tree.total_priority\n",
    "            \n",
    "            #  IS = (1/N * 1/P(i))**b /max wi == (N*P(i))**-b  /max wi\n",
    "            b_ISWeights[i, 0] = np.power(n * sampling_probabilities, -PER_b)/ max_weight\n",
    "                                   \n",
    "            b_idx[i]= index\n",
    "            \n",
    "            experience = [data]\n",
    "            \n",
    "            memory_b.append(experience)\n",
    "        \n",
    "        return b_idx, memory_b, b_ISWeights\n",
    "            \n",
    "        \n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        \"\"\"\n",
    "        Update the priorities on tree \n",
    "        \"\"\"\n",
    "        abs_errors += PER_e  # convert to abs and avoid 0\n",
    "        clipped_errors = np.minimum(abs_errors, self.abs_err)\n",
    "        ps = np.power(clipped_errors, PER_a)\n",
    "\n",
    "        for ti, p in zip(tree_idx, ps):\n",
    "            self.tree.update(ti, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#activate and populate memory \n",
    "memory = Memory(memory_size)\n",
    "\n",
    "#render env\n",
    "game.new_episode()\n",
    "\n",
    "#populate # of exp == pretrained_len\n",
    "for i in range(pretrained_len):\n",
    "    #initilize state for first step  \n",
    "    if i == 0: \n",
    "        state = game.get_state().screen_buffer #init state \n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "    #take a random action\n",
    "    action = random.choice(possible_actions)\n",
    "    \n",
    "    #reward from chosen action \n",
    "    reward = game.make_action(action)\n",
    "    \n",
    "    #check if done \n",
    "    done = game.is_episode_finished()\n",
    "    \n",
    "    if not done:\n",
    "        #get next state \n",
    "        next_state = game.get_state().screen_buffer\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "      \n",
    "        #add memory\n",
    "        exp = state, action, reward, next_state, done\n",
    "        memory.add(exp)\n",
    "        \n",
    "        #update state\n",
    "        state = next_state\n",
    "        \n",
    "    else: #were dead\n",
    "        #update next state \n",
    "        next_state = np.zeros(state.shape)\n",
    "        \n",
    "        #add exp \n",
    "        exp = state, action, reward, next_state, done\n",
    "        memory.add(exp)\n",
    "\n",
    "        #new episode and restart \n",
    "        game.new_episode()\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed TD Targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_weights(): \n",
    "    \"\"\"\n",
    "    returns list of weights mapping \n",
    "    \"\"\"\n",
    "    \n",
    "    new_ws = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"DQNetwork\")\n",
    "    old_ws = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"TargetNetwork\")\n",
    "\n",
    "    #op_holder will map new_ws to old_ws\n",
    "    op_holder = [] \n",
    "    for old_w, new_w in zip(old_ws, new_ws):\n",
    "        op_holder.append(new_w.assign(old_w))\n",
    "    return op_holder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to predict action based on epsilon greedy \n",
    "\n",
    "def explore_exploit(epsilon, min_eps, decay_rate, decay_step, state, num_actions):\n",
    "    #produce random number between 0 and 1 \n",
    "    check = np.random.rand()\n",
    "    explore_prob = min_eps + (epsilon-min_eps)*np.exp(-decay_rate*decay_step)\n",
    "    \n",
    "    #explore\n",
    "    if explore_prob>check:\n",
    "        #take random action \n",
    "        action = random.choice(possible_actions)  \n",
    "        \n",
    "    #exploit \n",
    "    else:\n",
    "        #q vals predicted by network \n",
    "        Qs = sess.run(DQNetwork.out, feed_dict = {DQNetwork.inputs : state.reshape((1,*state.shape))})\n",
    "        #choose action corresponding to best Q value \n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[int(choice)]\n",
    "    return action, explore_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for saving \n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-d356f951aa71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;31m#get predicted Q's for each next state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mnext_qs_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDQNetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mDQNetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mnext_states_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;31m#determine if state is terminal and set value for target_q\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#training loop\n",
    "if training: \n",
    "    with tf.Session() as sess:\n",
    "        #init vars and decay step\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        decay_step = 0 \n",
    "        \n",
    "        #determine when to update target network weights \n",
    "        tau = 0 \n",
    "        \n",
    "        #update targets once before starting \n",
    "        update_target = update_target_weights()\n",
    "        sess.run(update_target)\n",
    "        \n",
    "        #rewards for each episode \n",
    "        episodic_rewards = [] \n",
    "        \n",
    "        #init doom  \n",
    "        game.init()\n",
    "        \n",
    "        for episode in range(total_episodes):\n",
    "            #init steps for episode \n",
    "            step = 0 \n",
    "            \n",
    "            #init observation\n",
    "            game.new_episode()\n",
    "            state = game.get_state().screen_buffer\n",
    "            state, stacked_frame = stack_frames(stacked_frames, state, True)\n",
    "            \n",
    "            #list for rewards collected in episode \n",
    "            rewards_list = []\n",
    "            \n",
    "            while step < max_steps:\n",
    "                ################SAMPLING#######################    \n",
    "                #increments \n",
    "                step += 1\n",
    "                decay_step += 1\n",
    "                tau += 1 \n",
    "                \n",
    "                #if step % 50 == 0: print (step)\n",
    "                #choose action \n",
    "                action, explore_prob = explore_exploit(epsilon, min_eps, decay_rate, decay_step, state, num_actions)\n",
    "\n",
    "                #take action                 \n",
    "                reward = game.make_action(action)inputs_: state.reshape((1, *state.shape))})\n",
    "                \n",
    "                #check if game is done \n",
    "                done = game.is_episode_finished()\n",
    "                \n",
    "                rewards_list.append(reward) \n",
    "                \n",
    "                #if were not dead \n",
    "                if not done:\n",
    "                    #get next state\n",
    "                    next_state = game.get_state().screen_buffer\n",
    "                    \n",
    "                    #store transition after converting to proper format \n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "                    #add memory \n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                    \n",
    "                    #update state\n",
    "                    state = next_state\n",
    "\n",
    "                #if were dead     \n",
    "                else: \n",
    "                    #update next state \n",
    "                    next_state = np.zeros_like(state)\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    \n",
    "                    #add episodic rewards \n",
    "                    tot_reward = np.sum(rewards_list)\n",
    "                    episodic_rewards.append((episode, tot_reward))\n",
    "                    \n",
    "                    #print episode summary \n",
    "                    print(  'Episode: {}'.format(episode),\n",
    "                            'Total reward: {}'.format(tot_reward),\n",
    "                            'Explore P: {:.4f}'.format(explore_prob),\n",
    "                            'Training Loss {:.4f}'.format(loss))\n",
    "                    \n",
    "                    #end episode \n",
    "                    step = max_steps\n",
    "                    \n",
    "                    #add exp \n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "                ################Learning#######################    \n",
    "                #extract minibatch values \n",
    "                mini_batch = memory.sample(bs)\n",
    "                states_batch = np.array([sample[0] for sample in mini_batch])\n",
    "                actions_batch = np.array([sample[1] for sample in mini_batch])\n",
    "                rewards_batch = np.array([sample[2] for sample in mini_batch])\n",
    "                next_states_batch = np.array([sample[3] for sample in mini_batch])\n",
    "                dones_batch = np.array([sample[4] for sample in mini_batch])\n",
    "                \n",
    "                #target qs to be set below \n",
    "                targ_qs_batch = []\n",
    "                \n",
    "                #get predicted Q's for each next state in the batch \n",
    "                #For double DQNs we use DQN to determine action for next state \n",
    "                dqn_next_qs_batch = sess.run(DQNetwork.out, feed_dict={DQNetwork.inputs : next_states_batch})\n",
    "                \n",
    "                #calculate q values for next state with target_network \n",
    "                targetnet_qs_batch = sess.run(TargetNetwork.out, feed_dict={TargetNetwork.inputs : next_states_batch})\n",
    "                \n",
    "                #determine if state is terminal and set value for target_q\n",
    "                for i in range(bs):\n",
    "                    final_state = dones_batch[i]\n",
    "                    \n",
    "                    if final_state:\n",
    "                        targ_qs_batch.append(rewards_batch[i])\n",
    "                    \n",
    "                    else:   \n",
    "                        action = np.argmax(dqn_next_qs_batch[i])\n",
    "                        target_q = rewards_batch[i] + gamma * targetnet_qs_batch[i][action]\n",
    "                        targ_qs_batch.append(target_q)\n",
    "                    \n",
    "                \n",
    "                #convert to np array \n",
    "                target_q = np.array([i for i in targ_qs_batch])\n",
    "                \n",
    "                #determine loss \n",
    "                loss, _ = sess.run([DQNetwork.loss, DQNetwork.optim], feed_dict={DQNetwork.inputs: states_batch,\n",
    "                                                                               DQNetwork.target_Q: targ_qs_batch,\n",
    "                                                                               DQNetwork.actions: actions_batch})\n",
    "                #if tau > max_tau update target network weights \n",
    "                if tau > max_tau:\n",
    "                    update_targ = update_target_weights()\n",
    "                    sess.run(update_targ)\n",
    "                    tau = 0 \n",
    "                    print (\"Model Updated\")\n",
    "                    \n",
    "            if episode % 5 == 0:\n",
    "                save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "                print(\"Model Saved\")              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0fa5fd2e2e7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_episodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#setup env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "test_episodes = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    #setup env \n",
    "    game, possible_actions = init_env()\n",
    "    \n",
    "    #load the model \n",
    "    saver.restore(sess, \"./models/model.ckpt\")\n",
    "    game.init()\n",
    "    total_score = 0 \n",
    "\n",
    "    #run for each episode  \n",
    "    for episode in range(test_episodes):\n",
    "             \n",
    "        game.new_episode()\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "        while not game.is_episode_finished():\n",
    "            #choose action corresponding to best q val \n",
    "            q_preds = sess.run(DQNetwork.out, feed_dict={DQNetwork.inputs:state.reshape((1,*state.shape))})            \n",
    "            choice = np.argmax(q_preds)\n",
    "            action = possible_actions[int(choice)]\n",
    "            \n",
    "            #take action\n",
    "            game.make_action(action)\n",
    "            done = game.is_episode_finished()\n",
    "            score = game.get_total_reward()\n",
    "            \n",
    "            if done:  #if game is done  \n",
    "                break \n",
    "            \n",
    "            else:\n",
    "                next_state = game.get_state().screen_buffer\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                state = next_state \n",
    "            \n",
    "        print (\"\"\">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
    "                    Total Reward: {}\"\"\".format(game.get_total_reward())) \n",
    "        total_score += score\n",
    "    \n",
    "                 \n",
    "    game.close()\n",
    "    print (\"****************************\")\n",
    "    print (\"Average Reward over {} episodes:\\t\".format(test_episodes), total_score/float(test_episodes))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
