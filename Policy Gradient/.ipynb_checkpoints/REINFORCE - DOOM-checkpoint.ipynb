{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from vizdoom import *\n",
    "\n",
    "\n",
    "from skimage import transform \n",
    "from skimage.color import rgb2gray \n",
    "\n",
    "from collections import deque \n",
    "\n",
    "import time \n",
    "import random \n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_env():\n",
    "    game = DoomGame()\n",
    "    game.load_config(\"basic.cfg\")\n",
    "    game.set_doom_scenario_path(\"basic.wad\")\n",
    "    game.init()\n",
    "    \n",
    "    left = [1,0,0]\n",
    "    shoot = [0,0,1]\n",
    "    right = [0,1,0]\n",
    "    \n",
    "    possible_actions = [left, right, shoot]\n",
    "    \n",
    "    return game, possible_actions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call init_env \n",
    "game, possible_actions = init_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    \"\"\"\n",
    "    Convert frames to grayscale, crop irrelevent parts, normalize pixels, \n",
    "    resize frame.\n",
    "    \n",
    "    Input is frame of size 210x160x3\n",
    "    Returns frame of size 110x84x1 \n",
    "    \"\"\"\n",
    "    #graying done by vizdoom\n",
    "    #grayed = rgb2gray(frame)\n",
    "    \n",
    "    cropped = frame[30:-10, 30:-30]\n",
    "    normed = cropped/255.0 \n",
    "    resized = transform.resize(normed, [84,84])\n",
    "    \n",
    "    return resized #returns preprocessed frame \n",
    "\n",
    "def stack_frames(stacked_frames, frame, is_new_episode, stack_size=4):\n",
    "    \"\"\"\n",
    "    takes a frame/state and preprocesses it, \n",
    "    if same episode:\n",
    "        adds to the stack state \n",
    "    else new episode: \n",
    "        creates stacked state \n",
    "    \n",
    "    where the stacked state is 4 stacked states(frames)\n",
    "    returns stacked state where axis=1 is for different frames \n",
    "    \"\"\"\n",
    "    frame = preprocess_frame(frame)\n",
    "    \n",
    "    if not is_new_episode:\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "    \n",
    "    else: \n",
    "        #init deque \n",
    "        stacked_frames = deque([np.zeros((110,84), dtype=np.int) \n",
    "                                for i in range(stack_size)], maxlen=4)\n",
    "        #new episode so same frame x4 \n",
    "        for i in range(4): stacked_frames.append(frame)\n",
    "        \n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "\n",
    "    return stacked_state, stacked_frames\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_and_normalize_rewards(episode_rewards):\n",
    "    discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
    "    cumulative = 0.0\n",
    "    for i in reversed(range(len(episode_rewards))):\n",
    "        cumulative = cumulative * gamma + episode_rewards[i]\n",
    "        discounted_episode_rewards[i] = cumulative\n",
    "    \n",
    "    mean = np.mean(discounted_episode_rewards)\n",
    "    std = np.std(discounted_episode_rewards)\n",
    "    discounted_episode_rewards = (discounted_episode_rewards - mean) / (std)\n",
    "\n",
    "    return discounted_episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 84, 4) (4, 84, 84)\n"
     ]
    }
   ],
   "source": [
    "stack_size = 4\n",
    "\n",
    "#init frame stack\n",
    "stacked_frames  =  deque([np.zeros((110,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "frame = np.zeros((110,84), dtype=np.int)\n",
    "\n",
    "#test helper functions \n",
    "sample_state, stacked_frames = stack_frames(stacked_frames, frame, True)\n",
    "print(np.shape(sample_state), np.shape(stacked_frames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training params  \n",
    "total_episodes = 5000 #total num of episodes \n",
    "total_test_episodes = 10 #total num of episodes to test on \n",
    "max_steps = 5000 #max num of steps per episode \n",
    "bs = 64\n",
    "\n",
    "#network params \n",
    "lr = 0.0002 #learning rate \n",
    "state_size = [84,84,4]  #4 84x84 frames \n",
    "action_size = game.get_available_buttons_size() # 3 actions\n",
    "\n",
    "#Fixed Targets Params \n",
    "max_tau = 10000\n",
    "\n",
    "#discount factor \n",
    "gamma = 0.95 \n",
    "\n",
    "#exploration params \n",
    "epsilon = 1.0 #starting value for eps greedy (explore)\n",
    "max_eps = 1.0  #max value for eps greey \n",
    "min_eps = 0.01 #min value for eps greedy \n",
    "decay_rate = 0.0001 #decay rate for eps \n",
    "\n",
    "#Recall Params\n",
    "memory_size = 1000000\n",
    "pretrained_len = bs #number of init memories \n",
    "\n",
    "#Training Mode \n",
    "training=True \n",
    "\n",
    "#Env should be rendered or not \n",
    "episode_render = True \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE: Simple Model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy_Gradient():\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='DQN', training=training):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.lr = learning_rate\n",
    "        self.training = training\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            #placeholders\n",
    "            self.inputs = tf.placeholder(tf.float32,[None, *self.state_size])\n",
    "            self.actions = tf.placeholder(tf.float32, [None, self.action_size])\n",
    "            self.target_Q = tf.placeholder(tf.float32)\n",
    "            self.discounted_eps_rewards = tf.placeholder(tf.float32, [None,])\n",
    "\n",
    "\n",
    "            #block 1 \n",
    "            self.conv1 = tf.layers.conv2d(inputs=self.inputs, filters=32, \n",
    "                                          kernel_size=[8,8], \n",
    "                                          strides=(4,4), \n",
    "                                          padding='valid', \n",
    "                                          kernel_initializer= tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            self.bn1 = tf.layers.batch_normalization(self.conv1, epsilon=1e-5, training=self.training)\n",
    "            self.elu1 = tf.nn.elu(self.bn1)\n",
    "            \n",
    "            #block 2\n",
    "            self.conv2 = tf.layers.conv2d(inputs=self.elu1, filters=64, \n",
    "                                          kernel_size=[4,4], \n",
    "                                          strides=(2,2), \n",
    "                                          padding='valid', \n",
    "                                          kernel_initializer= tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            self.bn2 = tf.layers.batch_normalization(self.conv2, epsilon=1e-5, training=self.training)\n",
    "            self.elu2 = tf.nn.elu(self.bn2)\n",
    "            \n",
    "            #block 3\n",
    "            self.conv3 = tf.layers.conv2d(inputs=self.elu2, filters=128, \n",
    "                                          kernel_size=[4,4], \n",
    "                                          strides=(2,2), \n",
    "                                          padding='valid', \n",
    "                                          kernel_initializer= tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            self.bn3 = tf.layers.batch_normalization(self.conv3, epsilon=1e-5, training=self.training)\n",
    "            self.elu3 = tf.nn.elu(self.bn3)\n",
    "            \n",
    "            #FC Block \n",
    "            self.flat = tf.layers.flatten(self.elu3)\n",
    "            self.fc = tf.layers.dense(self.flat, units=512, activation=tf.nn.elu, \n",
    "                                       kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.logits = tf.layers.dense(self.fc, units=3, activation=None, \n",
    "                                       kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            #dealing with logits \n",
    "            self.softmax = tf.nn.softmax(self.logits)\n",
    "            \n",
    "            self.neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits,\n",
    "                                                                          labels=self.action_size)\n",
    "            \n",
    "            #Loss Function \n",
    "            self.loss = tf.reduce_mean(self.neg_log_prob*self.discounted_eps_rewards)\n",
    "            \n",
    "            #Optimizer \n",
    "            self.optim = tf.train.RMSPropOptimizer(learning_rate=self.lr).minimize(self.loss)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "num_actions = len(possible_actions)\n",
    "\n",
    "#init the two networks \n",
    "#predictive network for updating weights \n",
    "REINFORCE = Policy_Gradient(state_size, num_actions, lr, name=\"PolicyGradient\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Batch for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(batch_size, stacked_frames):\n",
    "    \"\"\"\n",
    "    Create a batch of size bs and return list of states, actions, rewards for batch, \n",
    "    discounted rewards, and the final episode \n",
    "    \"\"\"\n",
    "    state_list, actions_list, rewards_of_eps, rewards_of_batch, discounted_rewards = [],[],[],[],[]\n",
    "    episode_number = 1 \n",
    "    \n",
    "    #launch new episode \n",
    "    game.new_episode()\n",
    "    \n",
    "    #get new state \n",
    "    state = game.get_state().screen_buffer\n",
    "    state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "    while True: \n",
    "        #calculate output of nn\n",
    "        action_prob_dist = sess.run(REINFORCE.softmax, \n",
    "                                    feed_dict={REINFORCE.inputs:state.reshape(1, *state_size)})\n",
    "        \n",
    "        #choose stochastically \n",
    "        choice = np.random.choice(range(action_prob_dist.shape[1]), p=action_prob_dist.ravel())\n",
    "        action = possible_actions[choice]\n",
    "        \n",
    "        #take action and check if finished\n",
    "        reward = game.make_action(action)\n",
    "        done = game.is_episode_finished()\n",
    "        \n",
    "        #store results \n",
    "        state_list.append(state)\n",
    "        actions_list.append(action)\n",
    "        rewards_of_eps.append(reward)\n",
    "        \n",
    "        if not done:\n",
    "            #continue to next state \n",
    "            next_state = game.get_state.screen_buffer\n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, state, False)\n",
    "            state = next_state\n",
    "       \n",
    "        else:                         \n",
    "            #append info to lists     \n",
    "            rewards_of_batch.append(rewards_of_episode)\n",
    "            discounted_rewards.append(discount_and_normalize_rewards(rewards_of_episode))\n",
    "            \n",
    "            #reset stores \n",
    "            rewards_of_eps = []\n",
    "            \n",
    "            episode_number +=1\n",
    "            \n",
    "            #new episode \n",
    "            game.new_episode()\n",
    "            \n",
    "            #state and stack \n",
    "            next_state = game.get_state().screen_buffer\n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, True)\n",
    "            state = next_state \n",
    "    \n",
    "    return np.stack(np.array(states)), np.stack(np.array(actions)),np.concatenate(rewards_of_batch), np.concatenate(discounted_rewards), episode_number  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ViZDoomErrorException",
     "evalue": "Unexpected ViZDoom instance crash.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mViZDoomErrorException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-6acc7d7509ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0meps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m# Gather training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mstates_mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions_mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards_of_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscounted_rewards_mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_episodes_mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacked_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m### These part is used for analytics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-0aedfc609780>\u001b[0m in \u001b[0;36mcreate_batch\u001b[0;34m(batch_size, stacked_frames)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#launch new episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#get new state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mViZDoomErrorException\u001b[0m: Unexpected ViZDoom instance crash."
     ]
    }
   ],
   "source": [
    "# Keep track of all rewards total for each batch\n",
    "allRewards = []\n",
    "total_rewards = 0\n",
    "maximumRewardRecorded = 0\n",
    "mean_reward_total = []\n",
    "epoch = 1\n",
    "average_reward = []\n",
    "\n",
    "#saver\n",
    "#saver = tf.train.Saver()\n",
    "\n",
    "#training loop\n",
    "if training: \n",
    "    with tf.Session() as sess:\n",
    "        for eps in range(total_episodes):\n",
    "            # Gather training data\n",
    "            states_mb, actions_mb, rewards_of_batch, discounted_rewards_mb, nb_episodes_mb = create_batch(bs, stacked_frames)\n",
    "\n",
    "            ### These part is used for analytics\n",
    "            # Calculate the total reward ot the batch\n",
    "            total_reward_of_that_batch = np.sum(rewards_of_batch)\n",
    "            allRewards.append(total_reward_of_that_batch)\n",
    "\n",
    "            # Calculate the mean reward of the batch\n",
    "            # Total rewards of batch / nb episodes in that batch\n",
    "            mean_reward_of_that_batch = np.divide(total_reward_of_that_batch, nb_episodes_mb)\n",
    "            mean_reward_total.append(mean_reward_of_that_batch)\n",
    "            \n",
    "            # Calculate the average reward of all training\n",
    "            # mean_reward_of_that_batch / epoch\n",
    "            average_reward_of_all_training = np.divide(np.sum(mean_reward_total), epoch)\n",
    "\n",
    "            # Calculate maximum reward recorded \n",
    "            maximumRewardRecorded = np.amax(allRewards)\n",
    "\n",
    "            print(\"==========================================\")\n",
    "            print(\"Epoch: \", epoch, \"/\", num_epochs)\n",
    "            print(\"-----------\")\n",
    "            print(\"Number of training episodes: {}\".format(nb_episodes_mb))\n",
    "            print(\"Total reward: {}\".format(total_reward_of_that_batch, nb_episodes_mb))\n",
    "            print(\"Mean Reward of that batch {}\".format(mean_reward_of_that_batch))\n",
    "            print(\"Average Reward of all training: {}\".format(average_reward_of_all_training))\n",
    "            print(\"Max reward for a batch so far: {}\".format(maximumRewardRecorded))\n",
    "\n",
    "            # Feedforward, gradient and backpropagation\n",
    "            loss_, _ = sess.run([REINFORCE.loss, REINFORCE.optim], \n",
    "                                feed_dict={REINFORCE.inputs: states_mb.reshape((len(states_mb), 84,84,4)),\n",
    "                                           REINFORCE.actions: actions_mb,\n",
    "                            REINFORCE.discounted_episode_rewards_: discounted_rewards_mb \n",
    "                                                                        })\n",
    "\n",
    "            print(\"Training Loss: {}\".format(loss_))\n",
    "\n",
    "\n",
    "            if eps % 5 == 0:\n",
    "                save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "                print(\"Model Saved\")              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0fa5fd2e2e7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_episodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#setup env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "test_episodes = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    #setup env \n",
    "    game, possible_actions = init_env()\n",
    "    \n",
    "    #load the model \n",
    "    saver.restore(sess, \"./models/model.ckpt\")\n",
    "    game.init()\n",
    "    total_score = 0 \n",
    "\n",
    "    #run for each episode  \n",
    "    for episode in range(test_episodes):\n",
    "             \n",
    "        game.new_episode()\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "        while not game.is_episode_finished():\n",
    "            #choose action corresponding to best q val \n",
    "            q_preds = sess.run(DQNetwork.out, feed_dict={DQNetwork.inputs:state.reshape((1,*state.shape))})            \n",
    "            choice = np.argmax(q_preds)\n",
    "            action = possible_actions[int(choice)]\n",
    "            \n",
    "            #take action\n",
    "            game.make_action(action)\n",
    "            done = game.is_episode_finished()\n",
    "            score = game.get_total_reward()\n",
    "            \n",
    "            if done:  #if game is done  \n",
    "                break \n",
    "            \n",
    "            else:\n",
    "                next_state = game.get_state().screen_buffer\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                state = next_state \n",
    "            \n",
    "        print (\"\"\">>>>>>>>>>>>>>>>>>>> TESTING SUMMARY >>>>>>>>>>>>>>>>>>>>\n",
    "                    Total Reward: {}\"\"\".format(game.get_total_reward())) \n",
    "        total_score += score\n",
    "    \n",
    "                 \n",
    "    game.close()\n",
    "    print (\"****************************\")\n",
    "    print (\"Average Reward over {} episodes:\\t\".format(test_episodes), total_score/float(test_episodes))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
